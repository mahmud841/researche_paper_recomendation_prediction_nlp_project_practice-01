{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffffd36a-55aa-4e4b-b227-2f681ec0ed5b",
   "metadata": {},
   "source": [
    "# Loading Tools and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bb3f322-135c-4d4b-9f65-d0cd890327ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2caa13e-1c3a-48a8-bc8f-1d73aa8c980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ast import literal_eval\n",
    "# is used for safely evaluating strings containing Python literals or container displays\n",
    "# (e.g., lists, dictionaries) to their corresponding Python objects.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "807f5a8d-9856-40da-b8a8-0841cd3d4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arxiv_data = pd.read_csv(\"arxiv_data.csv\")\n",
    "arxiv_data = pd.read_csv(\"arxiv_data_210930-054931.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f05b218a-ef2f-420d-a380-4a8c7c869b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Multi-Level Attention Pooling for Graph Neural...</td>\n",
       "      <td>Graph neural networks (GNNs) have been widely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['cs.LG', 'cs.AI']</td>\n",
       "      <td>Decision Forests vs. Deep Networks: Conceptual...</td>\n",
       "      <td>Deep networks and decision forests (such as ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['cs.LG', 'cs.CR', 'stat.ML']</td>\n",
       "      <td>Power up! Robust Graph Convolutional Network v...</td>\n",
       "      <td>Graph convolutional networks (GCNs) are powerf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['cs.LG', 'cs.CR']</td>\n",
       "      <td>Releasing Graph Neural Networks with Different...</td>\n",
       "      <td>With the increasing popularity of Graph Neural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Recurrence-Aware Long-Term Cognitive Network f...</td>\n",
       "      <td>Machine learning solutions for pattern classif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           terms  \\\n",
       "0                      ['cs.LG']   \n",
       "1             ['cs.LG', 'cs.AI']   \n",
       "2  ['cs.LG', 'cs.CR', 'stat.ML']   \n",
       "3             ['cs.LG', 'cs.CR']   \n",
       "4                      ['cs.LG']   \n",
       "\n",
       "                                              titles  \\\n",
       "0  Multi-Level Attention Pooling for Graph Neural...   \n",
       "1  Decision Forests vs. Deep Networks: Conceptual...   \n",
       "2  Power up! Robust Graph Convolutional Network v...   \n",
       "3  Releasing Graph Neural Networks with Different...   \n",
       "4  Recurrence-Aware Long-Term Cognitive Network f...   \n",
       "\n",
       "                                           abstracts  \n",
       "0  Graph neural networks (GNNs) have been widely ...  \n",
       "1  Deep networks and decision forests (such as ra...  \n",
       "2  Graph convolutional networks (GCNs) are powerf...  \n",
       "3  With the increasing popularity of Graph Neural...  \n",
       "4  Machine learning solutions for pattern classif...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d4b85f4-23fe-4179-ba02-521428d66402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arxiv_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bb160a9-2e14-45d7-af2b-7dbbfd7a0649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arxiv_data.drop(columns = [\"terms\",\"abstracts\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76b0820a-97cb-4dec-bbc9-eec2bd4873bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arxiv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c8d10-825a-4428-a42c-ee267ad5a912",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fdfe9f5-6318-42b8-ba2c-fe30595039ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56181, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ccb13b32-1216-46db-987d-e152909b54e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "terms        0\n",
       "titles       0\n",
       "abstracts    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ceebf22-1642-4b5c-aef9-761cfcc7746b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15054"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd44fab4-b22f-4e64-ab6e-5431d244b5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                          ['cs.LG']\n",
       "1                                 ['cs.LG', 'cs.AI']\n",
       "2                      ['cs.LG', 'cs.CR', 'stat.ML']\n",
       "3                                 ['cs.LG', 'cs.CR']\n",
       "4                                          ['cs.LG']\n",
       "                            ...                     \n",
       "56176                             ['cs.CV', 'cs.IR']\n",
       "56177    ['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']\n",
       "56178                                      ['cs.LG']\n",
       "56179                ['stat.ML', 'cs.LG', 'math.OC']\n",
       "56180                  ['cs.LG', 'cs.AI', 'stat.ML']\n",
       "Name: terms, Length: 56181, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# o index data\n",
    "arxiv_data['terms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d01e5e23-5da0-4200-86ca-c887d78d2c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Graph neural networks (GNNs) have been widely ...\n",
       "1        Deep networks and decision forests (such as ra...\n",
       "2        Graph convolutional networks (GCNs) are powerf...\n",
       "3        With the increasing popularity of Graph Neural...\n",
       "4        Machine learning solutions for pattern classif...\n",
       "                               ...                        \n",
       "56176    Despite the growing availability of big data i...\n",
       "56177    This paper presents a simple end-to-end model ...\n",
       "56178    The popular Q-learning algorithm is known to o...\n",
       "56179    Principal components analysis (PCA) is a well-...\n",
       "56180    SDYNA is a general framework designed to addre...\n",
       "Name: abstracts, Length: 56181, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 index \n",
    "arxiv_data['abstracts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5f46cc0-e50c-4538-8eb0-8ff137e4ff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels : ['cs.LG' 'cs.AI' 'cs.CR' ... 'D.1.3; G.4; I.2.8; I.2.11; I.5.3; J.3'\n",
      " '68T07, 68T45, 68T10, 68T50, 68U35' 'I.2.0; G.3']\n",
      "lenght : 1177\n"
     ]
    }
   ],
   "source": [
    "labels_column = arxiv_data['terms'].apply(literal_eval)\n",
    "labels = labels_column.explode().unique()\n",
    "print(\"labels :\",labels)\n",
    "print(\"lenght :\",len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3cc45f0a-de71-482d-9d24-46d407c74df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>['cs.LG', 'cs.AI']</td>\n",
       "      <td>Learnable Hypergraph Laplacian for Hypergraph ...</td>\n",
       "      <td>HyperGraph Convolutional Neural Networks (HGCN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>['cs.LG', 'cs.AI']</td>\n",
       "      <td>Do Transformers Really Perform Bad for Graph R...</td>\n",
       "      <td>The Transformer architecture has become a domi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Self-supervised Auxiliary Learning for Graph N...</td>\n",
       "      <td>In recent years, graph neural networks (GNNs) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>['cs.LG', 'cs.CL', 'cs.SE', 'stat.ML']</td>\n",
       "      <td>Structured Neural Summarization</td>\n",
       "      <td>Summarization of long sequences into a concise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>['cs.CV', 'cs.LG', 'eess.IV']</td>\n",
       "      <td>Learning Local Neighboring Structure for Robus...</td>\n",
       "      <td>Mesh is a powerful data structure for 3D shape...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56171</th>\n",
       "      <td>['cs.LG', 'cs.AI']</td>\n",
       "      <td>Reinforcement Learning with Deep Energy-Based ...</td>\n",
       "      <td>We propose a method for learning expressive en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56172</th>\n",
       "      <td>['cs.LG', 'cs.AI']</td>\n",
       "      <td>A Laplacian Framework for Option Discovery in ...</td>\n",
       "      <td>Representation learning and option discovery a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56174</th>\n",
       "      <td>['cs.LG', 'stat.ML']</td>\n",
       "      <td>Neural Episodic Control</td>\n",
       "      <td>Deep reinforcement learning methods attain sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56178</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Deep Reinforcement Learning with Double Q-lear...</td>\n",
       "      <td>The popular Q-learning algorithm is known to o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56180</th>\n",
       "      <td>['cs.LG', 'cs.AI', 'stat.ML']</td>\n",
       "      <td>Chi-square Tests Driven Method for Learning th...</td>\n",
       "      <td>SDYNA is a general framework designed to addre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15076 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        terms  \\\n",
       "71                         ['cs.LG', 'cs.AI']   \n",
       "747                        ['cs.LG', 'cs.AI']   \n",
       "852                                 ['cs.LG']   \n",
       "950    ['cs.LG', 'cs.CL', 'cs.SE', 'stat.ML']   \n",
       "990             ['cs.CV', 'cs.LG', 'eess.IV']   \n",
       "...                                       ...   \n",
       "56171                      ['cs.LG', 'cs.AI']   \n",
       "56172                      ['cs.LG', 'cs.AI']   \n",
       "56174                    ['cs.LG', 'stat.ML']   \n",
       "56178                               ['cs.LG']   \n",
       "56180           ['cs.LG', 'cs.AI', 'stat.ML']   \n",
       "\n",
       "                                                  titles  \\\n",
       "71     Learnable Hypergraph Laplacian for Hypergraph ...   \n",
       "747    Do Transformers Really Perform Bad for Graph R...   \n",
       "852    Self-supervised Auxiliary Learning for Graph N...   \n",
       "950                      Structured Neural Summarization   \n",
       "990    Learning Local Neighboring Structure for Robus...   \n",
       "...                                                  ...   \n",
       "56171  Reinforcement Learning with Deep Energy-Based ...   \n",
       "56172  A Laplacian Framework for Option Discovery in ...   \n",
       "56174                            Neural Episodic Control   \n",
       "56178  Deep Reinforcement Learning with Double Q-lear...   \n",
       "56180  Chi-square Tests Driven Method for Learning th...   \n",
       "\n",
       "                                               abstracts  \n",
       "71     HyperGraph Convolutional Neural Networks (HGCN...  \n",
       "747    The Transformer architecture has become a domi...  \n",
       "852    In recent years, graph neural networks (GNNs) ...  \n",
       "950    Summarization of long sequences into a concise...  \n",
       "990    Mesh is a powerful data structure for 3D shape...  \n",
       "...                                                  ...  \n",
       "56171  We propose a method for learning expressive en...  \n",
       "56172  Representation learning and option discovery a...  \n",
       "56174  Deep reinforcement learning methods attain sup...  \n",
       "56178  The popular Q-learning algorithm is known to o...  \n",
       "56180  SDYNA is a general framework designed to addre...  \n",
       "\n",
       "[15076 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data[arxiv_data['titles'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31401f8a-ac06-4c38-aaeb-8d4f5544f2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56181, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26b04faf-8d65-4cc4-9396-7e0d78b9ae35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "terms\n",
       "['cs.CV']                                                         False\n",
       "['cs.LG', 'stat.ML']                                              False\n",
       "['cs.LG']                                                         False\n",
       "['cs.CV', 'cs.LG']                                                False\n",
       "['cs.LG', 'cs.AI']                                                False\n",
       "                                                                  ...  \n",
       "['cs.CV', 'I.4.9; J.3']                                            True\n",
       "['cs.LG', 'stat.ML', '68-04']                                      True\n",
       "['cs.LG', 'math.RT', 'stat.ML']                                    True\n",
       "['cs.LG', 'nlin.CD', 'physics.data-an', 'q-bio.QM', 'stat.ML']     True\n",
       "['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']                        True\n",
       "Name: count, Length: 3402, dtype: bool"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arxiv_data['terms'].value_counts()\n",
    "arxiv_data['terms'].value_counts()==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d1d1313b-e7df-470d-8ef1-39c31d25a039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1846"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(arxiv_data['terms'].value_counts()==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13a879bf-48a1-4c3a-ba50-d51834d6e839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846\n",
      "3402\n"
     ]
    }
   ],
   "source": [
    "print(sum(arxiv_data['terms'].value_counts()==1))\n",
    "print(arxiv_data['terms'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5896922c-d412-45f3-ad37-98bef686514d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54335, 3)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the rare terms. (it keeps only those rows where the \"terms\" value occurs more than once in the original DataFrame.)\n",
    "arxiv_data_filtered = arxiv_data.groupby('terms').filter(lambda x: len(x) > 1)\n",
    "arxiv_data_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af1990b6-93d2-4cc8-b738-8e767ef95438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['cs.LG']), list(['cs.LG', 'cs.AI']),\n",
       "       list(['cs.LG', 'cs.CR', 'stat.ML'])], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data_filtered['terms'] = arxiv_data_filtered['terms'].apply(lambda x: literal_eval(x))\n",
    "arxiv_data_filtered['terms'].values[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7d217-f736-4ee3-94c3-a2affbb0bbdc",
   "metadata": {},
   "source": [
    "## Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e64a86e-df34-4713-b320-8ab6d5fc1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_split = 0.1\n",
    "# The stratify parameter ensures that the splitting is done in a way that preserves the same distribution of labels (terms) in both the training and test sets.\n",
    "train_df, test_df = train_test_split(arxiv_data_filtered,test_size=0.1,stratify=arxiv_data_filtered[\"terms\"].values,)\n",
    "\n",
    "# Splitting the test set further into validation and new test sets.\n",
    "val_df = test_df.sample(frac=0.5)\n",
    "test_df.drop(val_df.index, inplace=True)\n",
    "\n",
    "# print(f\"Number of rows in training set: {len(train_df)}\")\n",
    "# print(f\"Number of rows in validation set: {len(val_df)}\")\n",
    "# print(f\"Number of rows in test set: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c4d82464-d55e-4995-986a-cdbb7e21fac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48901, 3), (2717, 3))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c87e7a0-bcbc-4e84-8f47-2e85a347cc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2717, 3), (2717, 3))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d470bc8d-2492-4b5e-9330-ff45d96e5bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48901, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9fc09723-c2a1-4da7-a3b3-30b49f552cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tf.ragged.constant(train_df['terms'])\n",
    "lookup = tf.keras.layers.StringLookup(output_mode='multi_hot')\n",
    "lookup.adapt(terms)\n",
    "vocab = lookup.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "50fa0112-8976-4f2f-bbbc-14b97ad56528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['terms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ebe4a43c-db87-4bfe-82ae-4e68c62aa412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cs.LG', 'stat.ML']\n",
      "tf.Tensor(\n",
      "[[0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 497), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# sample_label = train_df['terms'].iloc[0]\n",
    "# sample_label = train_df['terms'].iloc[1]\n",
    "sample_label = train_df['terms'].iloc[2]\n",
    "print(sample_label)\n",
    "label_binarized = lookup([sample_label])\n",
    "print(label_binarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "836870cd-f4df-4063-b976-8a1d11103458",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen = 150\n",
    "batch_size = 128\n",
    "padding_token = \"<pad>\"\n",
    "auto = tf.data.AUTOTUNE\n",
    "\n",
    "def make_dataset(dataframe, is_train=True):\n",
    "    labels = tf.ragged.constant(dataframe[\"terms\"].values)\n",
    "    # label_binarized is a NumPy array.\n",
    "    label_binarized = lookup(labels).numpy()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataframe[\"abstracts\"].values, label_binarized))\n",
    "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "train_dataset = make_dataset(train_df, is_train=True)\n",
    "validation_dataset = make_dataset(val_df, is_train=False)\n",
    "test_dataset = make_dataset(test_df, is_train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "345b7cd5-d167-4d28-9d7b-6087133a9471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=string, numpy=\n",
       " array([b'The objective of active learning (AL) is to train classification models with\\nless number of labeled instances by selecting only the most informative\\ninstances for labeling. The AL algorithms designed for other data types such as\\nimages and text do not perform well on graph-structured data. Although a few\\nheuristics-based AL algorithms have been proposed for graphs, a principled\\napproach is lacking. In this paper, we propose MetAL, an AL approach that\\nselects unlabeled instances that directly improve the future performance of a\\nclassification model. For a semi-supervised learning problem, we formulate the\\nAL task as a bilevel optimization problem. Based on recent work in\\nmeta-learning, we use the meta-gradients to approximate the impact of\\nretraining the model with any unlabeled instance on the model performance.\\nUsing multiple graph datasets belonging to different domains, we demonstrate\\nthat MetAL efficiently outperforms existing state-of-the-art AL algorithms.',\n",
       "        b'With the advances of data-driven machine learning research, a wide variety of\\nprediction problems have been tackled. It has become critical to explore how\\nmachine learning and specifically deep learning methods can be exploited to\\nanalyse healthcare data. A major limitation of existing methods has been the\\nfocus on grid-like data; however, the structure of physiological recordings are\\noften irregular and unordered which makes it difficult to conceptualise them as\\na matrix. As such, graph neural networks have attracted significant attention\\nby exploiting implicit information that resides in a biological system, with\\ninteractive nodes connected by edges whose weights can be either temporal\\nassociations or anatomical junctions. In this survey, we thoroughly review the\\ndifferent types of graph architectures and their applications in healthcare. We\\nprovide an overview of these methods in a systematic manner, organized by their\\ndomain of application including functional connectivity, anatomical structure\\nand electrical-based analysis. We also outline the limitations of existing\\ntechniques and discuss potential directions for future research.',\n",
       "        b'Structure Learning for Bayesian network (BN) is an important problem with\\nextensive research. It plays central roles in a wide variety of applications in\\nAlibaba Group. However, existing structure learning algorithms suffer from\\nconsiderable limitations in real world applications due to their low efficiency\\nand poor scalability. To resolve this, we propose a new structure learning\\nalgorithm LEAST, which comprehensively fulfills our business requirements as it\\nattains high accuracy, efficiency and scalability at the same time. The core\\nidea of LEAST is to formulate the structure learning into a continuous\\nconstrained optimization problem, with a novel differentiable constraint\\nfunction measuring the acyclicity of the resulting graph. Unlike with existing\\nwork, our constraint function is built on the spectral radius of the graph and\\ncould be evaluated in near linear time w.r.t. the graph node size. Based on it,\\nLEAST can be efficiently implemented with low storage overhead. According to\\nour benchmark evaluation, LEAST runs 1 to 2 orders of magnitude faster than\\nstate of the art method with comparable accuracy, and it is able to scale on\\nBNs with up to hundreds of thousands of variables. In our production\\nenvironment, LEAST is deployed and serves for more than 20 applications with\\nthousands of executions per day. We describe a concrete scenario in a ticket\\nbooking service in Alibaba, where LEAST is applied to build a near real-time\\nautomatic anomaly detection and root error cause analysis system. We also show\\nthat LEAST unlocks the possibility of applying BN structure learning in new\\nareas, such as large-scale gene expression data analysis and explainable\\nrecommendation system.',\n",
       "        b'Vision based human pose estimation is an non-invasive technology for\\nHuman-Computer Interaction (HCI). Direct use of the hand as an input device\\nprovides an attractive interaction method, with no need for specialized sensing\\nequipment, such as exoskeletons, gloves etc, but a camera. Traditionally, HCI\\nis employed in various applications spreading in areas including manufacturing,\\nsurgery, entertainment industry and architecture, to mention a few. Deployment\\nof vision based human pose estimation algorithms can give a breath of\\ninnovation to these applications. In this letter, we present a novel\\nConvolutional Neural Network architecture, reinforced with a Self-Attention\\nmodule that it can be deployed on an embedded system, due to its lightweight\\nnature, with just 1.9 Million parameters. The source code and qualitative\\nresults are publicly available.',\n",
       "        b'Determining whether hypotensive patients in intensive care units (ICUs)\\nshould receive fluid bolus therapy (FBT) has been an extremely challenging task\\nfor intensive care physicians as the corresponding increase in blood pressure\\nhas been hard to predict. Our study utilized regression models and\\nattention-based recurrent neural network (RNN) algorithms and a multi-clinical\\ninformation system large-scale database to build models that can predict the\\nsuccessful response to FBT among hypotensive patients in ICUs. We investigated\\nboth time-aggregated modeling using logistic regression algorithms with\\nregularization and time-series modeling using the long short term memory\\nnetwork (LSTM) and the gated recurrent units network (GRU) with the attention\\nmechanism for clinical interpretability. Among all modeling strategies, the\\nstacked LSTM with the attention mechanism yielded the most predictable model\\nwith the highest accuracy of 0.852 and area under the curve (AUC) value of\\n0.925. The study results may help identify hypotensive patients in ICUs who\\nwill have sufficient blood pressure recovery after FBT.',\n",
       "        b\"Soft biometrics analysis is seen as an important research topic, given its\\nrelevance to various applications. However, even though it is frequently seen\\nas a solved task, it can still be very hard to perform in wild conditions,\\nunder varying image conditions, uncooperative poses, and occlusions.\\nConsidering the gender trait as our topic of study, we report an extensive\\nanalysis of the feasibility of its inference regarding image (resolution,\\nluminosity, and blurriness) and subject-based features (face and body keypoints\\nconfidence). Using three state-of-the-art datasets (PETA, PA-100K, RAP) and\\nfive Person Attribute Recognition models, we correlate feature analysis with\\ngender inference accuracy using the Shapley value, enabling us to perceive the\\nimportance of each image/subject-based feature. Furthermore, we analyze\\nface-based gender inference and assess the pose effect on it. Our results\\nsuggest that: 1) image-based features are more influential for low-quality\\ndata; 2) an increase in image quality translates into higher subject-based\\nfeature importance; 3) face-based gender inference accuracy correlates with\\nimage quality increase; and 4) subjects' frontal pose promotes an implicit\\nattention towards the face. The reported results are seen as a basis for\\nsubsequent developments of inference approaches in uncontrolled outdoor\\nenvironments, which typically correspond to visual surveillance conditions.\",\n",
       "        b'In this work, we present an application of domain randomization and\\ngenerative adversarial networks (GAN) to train a near real-time object detector\\nfor industrial electric parts, entirely in a simulated environment. Large scale\\navailability of labelled real world data is typically rare and difficult to\\nobtain in many industrial settings. As such here, only a few hundred of\\nunlabelled real images are used to train a Cyclic-GAN network, in combination\\nwith various degree of domain randomization procedures. We demonstrate that\\nthis enables robust translation of synthetic images to the real world domain.\\nWe show that a combination of the original synthetic (simulation) and GAN\\ntranslated images, when used for training a Mask-RCNN object detection network\\nachieves greater than 0.95 mean average precision in detecting and classifying\\na collection of industrial electric parts. We evaluate the performance across\\ndifferent combinations of training data.',\n",
       "        b'Data preparation, i.e. the process of transforming raw data into a format\\nthat can be used for training effective machine learning models, is a tedious\\nand time-consuming task. For image data, preprocessing typically involves a\\nsequence of basic transformations such as cropping, filtering, rotating or\\nflipping images. Currently, data scientists decide manually based on their\\nexperience which transformations to apply in which particular order to a given\\nimage data set. Besides constituting a bottleneck in real-world data science\\nprojects, manual image data preprocessing may yield suboptimal results as data\\nscientists need to rely on intuition or trial-and-error approaches when\\nexploring the space of possible image transformations and thus might not be\\nable to discover the most effective ones. To mitigate the inefficiency and\\npotential ineffectiveness of manual data preprocessing, this paper proposes a\\ndeep reinforcement learning framework to automatically discover the optimal\\ndata preprocessing steps for training an image classifier. The framework takes\\nas input sets of labeled images and predefined preprocessing transformations.\\nIt jointly learns the classifier and the optimal preprocessing transformations\\nfor individual images. Experimental results show that the proposed approach not\\nonly improves the accuracy of image classifiers, but also makes them\\nsubstantially more robust to noisy inputs at test time.',\n",
       "        b\"While egocentric video is becoming increasingly popular, browsing it is very\\ndifficult. In this paper we present a compact 3D Convolutional Neural Network\\n(CNN) architecture for long-term activity recognition in egocentric videos.\\nRecognizing long-term activities enables us to temporally segment (index) long\\nand unstructured egocentric videos. Existing methods for this task are based on\\nhand tuned features derived from visible objects, location of hands, as well as\\noptical flow.\\n  Given a sparse optical flow volume as input, our CNN classifies the camera\\nwearer's activity. We obtain classification accuracy of 89%, which outperforms\\nthe current state-of-the-art by 19%. Additional evaluation is performed on an\\nextended egocentric video dataset, classifying twice the amount of categories\\nthan current state-of-the-art. Furthermore, our CNN is able to recognize\\nwhether a video is egocentric or not with 99.2% accuracy, up by 24% from\\ncurrent state-of-the-art. To better understand what the network actually\\nlearns, we propose a novel visualization of CNN kernels as flow fields.\",\n",
       "        b'The dominant object detection approaches treat each dataset separately and\\nfit towards a specific domain, which cannot adapt to other domains without\\nextensive retraining. In this paper, we address the problem of designing a\\nuniversal object detection model that exploits diverse category granularity\\nfrom multiple domains and predict all kinds of categories in one system.\\nExisting works treat this problem by integrating multiple detection branches\\nupon one shared backbone network. However, this paradigm overlooks the crucial\\nsemantic correlations between multiple domains, such as categories hierarchy,\\nvisual similarity, and linguistic relationship. To address these drawbacks, we\\npresent a novel universal object detector called Universal-RCNN that\\nincorporates graph transfer learning for propagating relevant semantic\\ninformation across multiple datasets to reach semantic coherency. Specifically,\\nwe first generate a global semantic pool by integrating all high-level semantic\\nrepresentation of all the categories. Then an Intra-Domain Reasoning Module\\nlearns and propagates the sparse graph representation within one dataset guided\\nby a spatial-aware GCN. Finally, an InterDomain Transfer Module is proposed to\\nexploit diverse transfer dependencies across all domains and enhance the\\nregional feature representation by attending and transferring semantic contexts\\nglobally. Extensive experiments demonstrate that the proposed method\\nsignificantly outperforms multiple-branch models and achieves the\\nstate-of-the-art results on multiple object detection benchmarks (mAP: 49.1% on\\nCOCO).',\n",
       "        b\"We apply generative adversarial convolutional neural networks to the problem\\nof style transfer to underdrawings and ghost-images in x-rays of fine art\\npaintings with a special focus on enhancing their spatial resolution. We build\\nupon a neural architecture developed for the related problem of synthesizing\\nhigh-resolution photo-realistic image from semantic label maps. Our neural\\narchitecture achieves high resolution through a hierarchy of generators and\\ndiscriminator sub-networks, working throughout a range of spatial resolutions.\\nThis coarse-to-fine generator architecture can increase the effective\\nresolution by a factor of eight in each spatial direction, or an overall\\nincrease in number of pixels by a factor of 64. We also show that even just a\\nfew examples of human-generated image segmentations can greatly improve --\\nqualitatively and quantitatively -- the generated images. We demonstrate our\\nmethod on works such as Leonardo's Madonna of the carnation and the\\nunderdrawing in his Virgin of the rocks, which pose several special problems in\\nstyle transfer, including the paucity of representative works from which to\\nlearn and transfer style information.\",\n",
       "        b'Research on group activity recognition mostly leans on the standard\\ntwo-stream approach (RGB and Optical Flow) as their input features. Few have\\nexplored explicit pose information, with none using it directly to reason about\\nthe persons interactions. In this paper, we leverage the skeleton information\\nto learn the interactions between the individuals straight from it. With our\\nproposed method GIRN, multiple relationship types are inferred from independent\\nmodules, that describe the relations between the body joints pair-by-pair.\\nAdditionally to the joints relations, we also experiment with the previously\\nunexplored relationship between individuals and relevant objects (e.g.\\nvolleyball). The individuals distinct relations are then merged through an\\nattention mechanism, that gives more importance to those individuals more\\nrelevant for distinguishing the group activity. We evaluate our method in the\\nVolleyball dataset, obtaining competitive results to the state-of-the-art. Our\\nexperiments demonstrate the potential of skeleton-based approaches for modeling\\nmulti-person interactions.',\n",
       "        b'Knowledge distillation (KD) has been actively studied for image\\nclassification tasks in deep learning, aiming to improve the performance of a\\nstudent model based on the knowledge from a teacher model. However, there have\\nbeen very few efforts for applying KD in image regression with a scalar\\nresponse, and there is no KD method applicable to both tasks. Moreover,\\nexisting KD methods often require a practitioner to carefully choose or adjust\\nthe teacher and student architectures, making these methods less scalable in\\npractice. Furthermore, although KD is usually conducted in scenarios with\\nlimited labeled data, very few techniques are developed to alleviate such data\\ninsufficiency. To solve the above problems in an all-in-one manner, we propose\\nin this paper a unified KD framework based on conditional generative\\nadversarial networks (cGANs), termed cGAN-KD. Fundamentally different from\\nexisting KD methods, cGAN-KD distills and transfers knowledge from a teacher\\nmodel to a student model via cGAN-generated samples. This unique mechanism\\nmakes cGAN-KD suitable for both classification and regression tasks, compatible\\nwith other KD methods, and insensitive to the teacher and student\\narchitectures. Also, benefiting from the recent advances in cGAN methodology\\nand our specially designed subsampling and filtering procedures, cGAN-KD also\\nperforms well when labeled data are scarce. An error bound of a student model\\ntrained in the cGAN-KD framework is derived in this work, which theoretically\\nexplains why cGAN-KD takes effect and guides the implementation of cGAN-KD in\\npractice. Extensive experiments on CIFAR-10 and Tiny-ImageNet show that we can\\nincorporate state-of-the-art KD methods into the cGAN-KD framework to reach a\\nnew state of the art. Also, experiments on RC-49 and UTKFace demonstrate the\\neffectiveness of cGAN-KD in image regression tasks, where existing KD methods\\nare inapplicable.',\n",
       "        b'Recent deep generative models allow real-time generation of hair images from\\nsketch inputs. Existing solutions often require a user-provided binary mask to\\nspecify a target hair shape. This not only costs users extra labor but also\\nfails to capture complicated hair boundaries. Those solutions usually encode\\nhair structures via orientation maps, which, however, are not very effective to\\nencode complex structures. We observe that colored hair sketches already\\nimplicitly define target hair shapes as well as hair appearance and are more\\nflexible to depict hair structures than orientation maps. Based on these\\nobservations, we present SketchHairSalon, a two-stage framework for generating\\nrealistic hair images directly from freehand sketches depicting desired hair\\nstructure and appearance. At the first stage, we train a network to predict a\\nhair matte from an input hair sketch, with an optional set of non-hair strokes.\\nAt the second stage, another network is trained to synthesize the structure and\\nappearance of hair images from the input sketch and the generated matte. To\\nmake the networks in the two stages aware of long-term dependency of strokes,\\nwe apply self-attention modules to them. To train these networks, we present a\\nnew dataset containing thousands of annotated hair sketch-image pairs and\\ncorresponding hair mattes. Two efficient methods for sketch completion are\\nproposed to automatically complete repetitive braided parts and hair strokes,\\nrespectively, thus reducing the workload of users. Based on the trained\\nnetworks and the two sketch completion strategies, we build an intuitive\\ninterface to allow even novice users to design visually pleasing hair images\\nexhibiting various hair structures and appearance via freehand sketches. The\\nqualitative and quantitative evaluations show the advantages of the proposed\\nsystem over the existing or alternative solutions.',\n",
       "        b'Advancements in deep generative models have made it possible to synthesize\\nimages, videos and audio signals that are difficult to distinguish from natural\\nsignals, creating opportunities for potential abuse of these capabilities. This\\nmotivates the problem of tracking the provenance of signals, i.e., being able\\nto determine the original source of a signal. Watermarking the signal at the\\ntime of signal creation is a potential solution, but current techniques are\\nbrittle and watermark detection mechanisms can easily be bypassed by applying\\npost-processing transformations (cropping images, shifting pitch in the audio\\netc.). In this paper, we introduce ReSWAT (Resilient Signal Watermarking via\\nAdversarial Training), a framework for learning transformation-resilient\\nwatermark detectors that are able to detect a watermark even after a signal has\\nbeen through several post-processing transformations. Our detection method can\\nbe applied to domains with continuous data representations such as images,\\nvideos or sound signals. Experiments on watermarking image and audio signals\\nshow that our method can reliably detect the provenance of a signal, even if it\\nhas been through several post-processing transformations, and improve upon\\nrelated work in this setting. Furthermore, we show that for specific kinds of\\ntransformations (perturbations bounded in the L2 norm), we can even get formal\\nguarantees on the ability of our model to detect the watermark. We provide\\nqualitative examples of watermarked image and audio samples in\\nhttps://drive.google.com/open?id=1-yZ0WIGNu2Iez7UpXBjtjVgZu3jJjFga.',\n",
       "        b'Keypoint detection is an essential component for the object registration and\\nalignment. However, previous works mainly focused on how to register keypoints\\nunder arbitrary rigid transformations. Differently, in this work, we reckon\\nkeypoints under an information compression scheme to represent the whole\\nobject. Based on this, we propose UKPGAN, an unsupervised 3D keypoint detector\\nwhere keypoints are detected so that they could reconstruct the original object\\nshape. Two modules: GAN-based keypoint sparsity control and salient information\\ndistillation modules are proposed to locate those important keypoints.\\nExtensive experiments show that our keypoints preserve the semantic information\\nof objects and align well with human annotated part and keypoint labels.\\nFurthermore, we show that UKPGAN can be applied to either rigid objects or\\nnon-rigid SMPL human bodies under arbitrary pose deformations. As a keypoint\\ndetector, our model is stable under both rigid and non-rigid transformations,\\nwith local reference frame estimation. Our code is available on\\nhttps://github.com/qq456cvb/UKPGAN.',\n",
       "        b'Audio-visual learning, aimed at exploiting the relationship between audio and\\nvisual modalities, has drawn considerable attention since deep learning started\\nto be used successfully. Researchers tend to leverage these two modalities\\neither to improve the performance of previously considered single-modality\\ntasks or to address new challenging problems. In this paper, we provide a\\ncomprehensive survey of recent audio-visual learning development. We divide the\\ncurrent audio-visual learning tasks into four different subfields: audio-visual\\nseparation and localization, audio-visual correspondence learning, audio-visual\\ngeneration, and audio-visual representation learning. State-of-the-art methods\\nas well as the remaining challenges of each subfield are further discussed.\\nFinally, we summarize the commonly used datasets and performance metrics.',\n",
       "        b'We introduce a new graphical model for tracking radio-tagged animals and\\nlearning their movement patterns. The model provides a principled way to\\ncombine radio telemetry data with an arbitrary set of userdefined, spatial\\nfeatures. We describe an efficient stochastic gradient algorithm for fitting\\nmodel parameters to data and demonstrate its effectiveness via asymptotic\\nanalysis and synthetic experiments. We also apply our model to real datasets,\\nand show that it outperforms the most popular radio telemetry software package\\nused in ecology. We conclude that integration of different data sources under a\\nsingle statistical framework, coupled with appropriate parameter and state\\nestimation procedures, produces both accurate location estimates and an\\ninterpretable statistical model of animal movement.',\n",
       "        b'In order to successfully perform tasks specified by natural language\\ninstructions, an artificial agent operating in a visual world needs to map\\nwords, concepts, and actions from the instruction to visual elements in its\\nenvironment. This association is termed as Task-Oriented Grounding. In this\\nwork, we propose a novel Dynamic Attention Network architecture for the\\nefficient multi-modal fusion of text and visual representations which can\\ngenerate a robust definition of state for the policy learner. Our model assumes\\nno prior knowledge from visual and textual domains and is an end to end\\ntrainable. For a 3D visual world where the observation changes continuously,\\nthe attention on the visual elements tends to be highly co-related from a\\none-time step to the next. We term this as \"Dynamic Attention\". In this work,\\nwe show that Dynamic Attention helps in achieving grounding and also aids in\\nthe policy learning objective. Since most practical robotic applications take\\nplace in the real world where the observation space is continuous, our\\nframework can be used as a generalized multi-modal fusion unit for robotic\\ncontrol through natural language. We show the effectiveness of using 1D\\nconvolution over Gated Attention Hadamard product on the rate of convergence of\\nthe network. We demonstrate that the cell-state of a Long Short Term Memory\\n(LSTM) is a natural choice for modeling Dynamic Attention and shows through\\nvisualization that the generated attention is very close to how humans tend to\\nfocus on the environment.',\n",
       "        b'The existence of adversarial examples in which an imperceptible change in the\\ninput can fool well trained neural networks was experimentally discovered by\\nSzegedy et al in 2013, who called them \"Intriguing properties of neural\\nnetworks\". Since then, this topic had become one of the hottest research areas\\nwithin machine learning, but the ease with which we can switch between any two\\ndecisions in targeted attacks is still far from being understood, and in\\nparticular it is not clear which parameters determine the number of input\\ncoordinates we have to change in order to mislead the network. In this paper we\\ndevelop a simple mathematical framework which enables us to think about this\\nbaffling phenomenon from a fresh perspective, turning it into a natural\\nconsequence of the geometry of $\\\\mathbb{R}^n$ with the $L_0$ (Hamming) metric,\\nwhich can be quantitatively analyzed. In particular, we explain why we should\\nexpect to find targeted adversarial examples with Hamming distance of roughly\\n$m$ in arbitrarily deep neural networks which are designed to distinguish\\nbetween $m$ input classes.',\n",
       "        b'We propose a two-stage method for face hallucination. First, we generate\\nfacial components of the input image using CNNs. These components represent the\\nbasic facial structures. Second, we synthesize fine-grained facial structures\\nfrom high resolution training images. The details of these structures are\\ntransferred into facial components for enhancement. Therefore, we generate\\nfacial components to approximate ground truth global appearance in the first\\nstage and enhance them through recovering details in the second stage. The\\nexperiments demonstrate that our method performs favorably against\\nstate-of-the-art methods',\n",
       "        b'Scene text detection is an important step of scene text recognition system\\nand also a challenging problem. Different from general object detection, the\\nmain challenges of scene text detection lie on arbitrary orientations, small\\nsizes, and significantly variant aspect ratios of text in natural images. In\\nthis paper, we present an end-to-end trainable fast scene text detector, named\\nTextBoxes++, which detects arbitrary-oriented scene text with both high\\naccuracy and efficiency in a single network forward pass. No post-processing\\nother than an efficient non-maximum suppression is involved. We have evaluated\\nthe proposed TextBoxes++ on four public datasets. In all experiments,\\nTextBoxes++ outperforms competing methods in terms of text localization\\naccuracy and runtime. More specifically, TextBoxes++ achieves an f-measure of\\n0.817 at 11.6fps for 1024*1024 ICDAR 2015 Incidental text images, and an\\nf-measure of 0.5591 at 19.8fps for 768*768 COCO-Text images. Furthermore,\\ncombined with a text recognizer, TextBoxes++ significantly outperforms the\\nstate-of-the-art approaches for word spotting and end-to-end text recognition\\ntasks on popular benchmarks. Code is available at:\\nhttps://github.com/MhLiao/TextBoxes_plusplus',\n",
       "        b'We present the Colorization Transformer, a novel approach for diverse high\\nfidelity image colorization based on self-attention. Given a grayscale image,\\nthe colorization proceeds in three steps. We first use a conditional\\nautoregressive transformer to produce a low resolution coarse coloring of the\\ngrayscale image. Our architecture adopts conditional transformer layers to\\neffectively condition grayscale input. Two subsequent fully parallel networks\\nupsample the coarse colored low resolution image into a finely colored high\\nresolution image. Sampling from the Colorization Transformer produces diverse\\ncolorings whose fidelity outperforms the previous state-of-the-art on\\ncolorising ImageNet based on FID results and based on a human evaluation in a\\nMechanical Turk test. Remarkably, in more than 60% of cases human evaluators\\nprefer the highest rated among three generated colorings over the ground truth.\\nThe code and pre-trained checkpoints for Colorization Transformer are publicly\\navailable at\\nhttps://github.com/google-research/google-research/tree/master/coltran',\n",
       "        b'Understanding the functional architecture of the brain in terms of networks\\nis becoming increasingly common. In most fMRI applications functional networks\\nare assumed to be stationary, resulting in a single network estimated for the\\nentire time course. However recent results suggest that the connectivity\\nbetween brain regions is highly non-stationary even at rest. As a result, there\\nis a need for new brain imaging methodologies that comprehensively account for\\nthe dynamic (i.e., non-stationary) nature of the fMRI data. In this work we\\npropose the Smooth Incremental Graphical Lasso Estimation (SINGLE) algorithm\\nwhich estimates dynamic brain networks from fMRI data. We apply the SINGLE\\nalgorithm to functional MRI data from 24 healthy patients performing a\\nchoice-response task to demonstrate the dynamic changes in network structure\\nthat accompany a simple but attentionally demanding cognitive task. Using graph\\ntheoretic measures we show that the Right Inferior Frontal Gyrus, frequently\\nreported as playing an important role in cognitive control, dynamically changes\\nwith the task. Our results suggest that the Right Inferior Frontal Gyrus plays\\na fundamental role in the attention and executive function during cognitively\\ndemanding tasks and may play a key role in regulating the balance between other\\nbrain regions.',\n",
       "        b'Recently, crowd counting is a hot topic in crowd analysis. Many CNN-based\\ncounting algorithms attain good performance. However, these methods only focus\\non the local appearance features of crowd scenes but ignore the large-range\\npixel-wise contextual and crowd attention information. To remedy the above\\nproblems, in this paper, we introduce the Spatial-/Channel-wise Attention\\nModels into the traditional Regression CNN to estimate the density map, which\\nis named as \"SCAR\". It consists of two modules, namely Spatial-wise Attention\\nModel (SAM) and Channel-wise Attention Model (CAM). The former can encode the\\npixel-wise context of the entire image to more accurately predict density maps\\nat the pixel level. The latter attempts to extract more discriminative features\\namong different channels, which aids model to pay attention to the head region,\\nthe core of crowd scenes. Intuitively, CAM alleviates the mistaken estimation\\nfor background regions. Finally, two types of attention information and\\ntraditional CNN\\'s feature maps are integrated by a concatenation operation.\\nFurthermore, the extensive experiments are conducted on four popular datasets,\\nShanghai Tech Part A/B, GCC, and UCF_CC_50 Dataset. The results show that the\\nproposed method achieves state-of-the-art results.',\n",
       "        b'Neuro-inspired recurrent neural network algorithms, such as echo state\\nnetworks, are computationally lightweight and thereby map well onto untethered\\ndevices. The baseline echo state network algorithms are shown to be efficient\\nin solving small-scale spatio-temporal problems. However, they underperform for\\ncomplex tasks that are characterized by multi-scale structures. In this\\nresearch, an intrinsic plasticity-infused modular deep echo state network\\narchitecture is proposed to solve complex and multiple timescale temporal\\ntasks. It outperforms state-of-the-art for time series prediction tasks.',\n",
       "        b'Registering point clouds of dressed humans to parametric human models is a\\nchallenging task in computer vision. Traditional approaches often rely on\\nheavily engineered pipelines that require accurate manual initialization of\\nhuman poses and tedious post-processing. More recently, learning-based methods\\nare proposed in hope to automate this process. We observe that pose\\ninitialization is key to accurate registration but existing methods often fail\\nto provide accurate pose initialization. One major obstacle is that, regressing\\njoint rotations from point clouds or images of humans is still very\\nchallenging. To this end, we propose novel piecewise transformation fields\\n(PTF), a set of functions that learn 3D translation vectors to map any query\\npoint in posed space to its correspond position in rest-pose space. We combine\\nPTF with multi-class occupancy networks, obtaining a novel learning-based\\nframework that learns to simultaneously predict shape and per-point\\ncorrespondences between the posed space and the canonical space for clothed\\nhuman. Our key insight is that the translation vector for each query point can\\nbe effectively estimated using the point-aligned local features; consequently,\\nrigid per bone transformations and joint rotations can be obtained efficiently\\nvia a least-square fitting given the estimated point correspondences,\\ncircumventing the challenging task of directly regressing joint rotations from\\nneural networks. Furthermore, the proposed PTF facilitate canonicalized\\noccupancy estimation, which greatly improves generalization capability and\\nresults in more accurate surface reconstruction with only half of the\\nparameters compared with the state-of-the-art. Both qualitative and\\nquantitative studies show that fitting parametric models with poses initialized\\nby our network results in much better registration quality, especially for\\nextreme poses.',\n",
       "        b'Expensive bounding-box annotations have limited the development of object\\ndetection task. Thus, it is necessary to focus on more challenging task of\\nfew-shot object detection. It requires the detector to recognize objects of\\nnovel classes with only a few training samples. Nowadays, many existing popular\\nmethods based on meta-learning have achieved promising performance, such as\\nMeta R-CNN series. However, only a single category of support data is used as\\nthe attention to guide the detecting of query images each time. Their relevance\\nto each other remains unexploited. Moreover, a lot of recent works treat the\\nsupport data and query images as independent branch without considering the\\nrelationship between them. To address this issue, we propose a dynamic\\nrelevance learning model, which utilizes the relationship between all support\\nimages and Region of Interest (RoI) on the query images to construct a dynamic\\ngraph convolutional network (GCN). By adjusting the prediction distribution of\\nthe base detector using the output of this GCN, the proposed model can guide\\nthe detector to improve the class representation implicitly. Comprehensive\\nexperiments have been conducted on Pascal VOC and MS-COCO dataset. The proposed\\nmodel achieves the best overall performance, which shows its effectiveness of\\nlearning more generalized features. Our code is available at\\nhttps://github.com/liuweijie19980216/DRL-for-FSOD.',\n",
       "        b'Machine-learning algorithms offer immense possibilities in the development of\\nseveral cognitive applications. In fact, large scale machine-learning\\nclassifiers now represent the state-of-the-art in a wide range of object\\ndetection/classification problems. However, the network complexities of\\nlarge-scale classifiers present them as one of the most challenging and energy\\nintensive workloads across the computing spectrum. In this paper, we present a\\nnew approach to optimize energy efficiency of object detection tasks using\\nsemantic decomposition to build a hierarchical classification framework. We\\nobserve that certain semantic information like color/texture are common across\\nvarious images in real-world datasets for object detection applications. We\\nexploit these common semantic features to distinguish the objects of interest\\nfrom the remaining inputs (non-objects of interest) in a dataset at a lower\\ncomputational effort. We propose a 2-stage hierarchical classification\\nframework, with increasing levels of complexity, wherein the first stage is\\ntrained to recognize the broad representative semantic features relevant to the\\nobject of interest. The first stage rejects the input instances that do not\\nhave the representative features and passes only the relevant instances to the\\nsecond stage. Our methodology thus allows us to reject certain information at\\nlower complexity and utilize the full computational effort of a network only on\\na smaller fraction of inputs to perform detection. We use color and texture as\\ndistinctive traits to carry out several experiments for object detection. Our\\nexperiments on the Caltech101/CIFAR10 dataset show that the proposed method\\nyields 1.93x/1.46x improvement in average energy, respectively, over the\\ntraditional single classifier model.',\n",
       "        b'Low rank tensor ring model is powerful for image completion which recovers\\nmissing entries in data acquisition and transformation. The recently proposed\\ntensor ring (TR) based completion algorithms generally solve the low rank\\noptimization problem by alternating least squares method with predefined ranks,\\nwhich may easily lead to overfitting when the unknown ranks are set too large\\nand only a few measurements are available. In this paper, we present a Bayesian\\nlow rank tensor ring model for image completion by automatically learning the\\nlow rank structure of data. A multiplicative interaction model is developed for\\nthe low-rank tensor ring decomposition, where core factors are enforced to be\\nsparse by assuming their entries obey Student-T distribution. Compared with\\nmost of the existing methods, the proposed one is free of parameter-tuning, and\\nthe TR ranks can be obtained by Bayesian inference. Numerical Experiments,\\nincluding synthetic data, color images with different sizes and YaleFace\\ndataset B with respect to one pose, show that the proposed approach outperforms\\nstate-of-the-art ones, especially in terms of recovery accuracy.',\n",
       "        b'The method of deep learning has achieved excellent results in improving the\\nperformance of robotic grasping detection. However, the deep learning methods\\nused in general object detection are not suitable for robotic grasping\\ndetection. Current modern object detectors are difficult to strike a balance\\nbetween high accuracy and fast inference speed. In this paper, we present an\\nefficient and robust fully convolutional neural network model to perform\\nrobotic grasping pose estimation from an n-channel input image of the real\\ngrasping scene. The proposed network is a lightweight generative architecture\\nfor grasping detection in one stage. Specifically, a grasping representation\\nbased on Gaussian kernel is introduced to encode training samples, which\\nembodies the principle of maximum central point grasping confidence. Meanwhile,\\nto extract multi-scale information and enhance the feature discriminability, a\\nreceptive field block (RFB) is assembled to the bottleneck of our grasping\\ndetection architecture. Besides, pixel attention and channel attention are\\ncombined to automatically learn to focus on fusing context information of\\nvarying shapes and sizes by suppressing the noise feature and highlighting the\\ngrasping object feature. Extensive experiments on two public grasping datasets,\\nCornell and Jacquard demonstrate the state-of-the-art performance of our method\\nin balancing accuracy and inference speed. The network is an order of magnitude\\nsmaller than other excellent algorithms while achieving better performance with\\nan accuracy of 98.9$\\\\%$ and 95.6$\\\\%$ on the Cornell and Jacquard datasets,\\nrespectively.',\n",
       "        b'We investigate the problem of Language-Based Image Editing (LBIE). Given a\\nsource image and a natural language description, we want to generate a target\\nimage by editing the source image based on the description. We propose a\\ngeneric modeling framework for two sub-tasks of LBIE: language-based image\\nsegmentation and image colorization. The framework uses recurrent attentive\\nmodels to fuse image and language features. Instead of using a fixed step size,\\nwe introduce for each region of the image a termination gate to dynamically\\ndetermine after each inference step whether to continue extrapolating\\nadditional information from the textual description. The effectiveness of the\\nframework is validated on three datasets. First, we introduce a synthetic\\ndataset, called CoSaL, to evaluate the end-to-end performance of our LBIE\\nsystem. Second, we show that the framework leads to state-of-the-art\\nperformance on image segmentation on the ReferIt dataset. Third, we present the\\nfirst language-based colorization result on the Oxford-102 Flowers dataset.',\n",
       "        b'There is active research targeting local image manipulations that can fool\\ndeep neural networks (DNNs) into producing incorrect results. This paper\\nexamines a type of global image manipulation that can produce similar adverse\\neffects. Specifically, we explore how strong color casts caused by incorrectly\\napplied computational color constancy - referred to as white balance (WB) in\\nphotography - negatively impact the performance of DNNs targeting image\\nsegmentation and classification. In addition, we discuss how existing image\\naugmentation methods used to improve the robustness of DNNs are not well suited\\nfor modeling WB errors. To address this problem, a novel augmentation method is\\nproposed that can emulate accurate color constancy degradation. We also explore\\npre-processing training and testing images with a recent WB correction\\nalgorithm to reduce the effects of incorrectly white-balanced images. We\\nexamine both augmentation and pre-processing strategies on different datasets\\nand demonstrate notable improvements on the CIFAR-10, CIFAR-100, and ADE20K\\ndatasets.',\n",
       "        b\"Early prediction of cerebral palsy is essential as it leads to early\\ntreatment and monitoring. Deep learning has shown promising results in\\nbiomedical engineering thanks to its capacity of modelling complicated data\\nwith its non-linear architecture. However, due to their complex structure, deep\\nlearning models are generally not interpretable by humans, making it difficult\\nfor clinicians to rely on the findings. In this paper, we propose a channel\\nattention module for deep learning models to predict cerebral palsy from\\ninfants' body movements, which highlights the key features (i.e. body joints)\\nthe model identifies as important, thereby indicating why certain diagnostic\\nresults are found. To highlight the capacity of the deep network in modelling\\ninput features, we utilize raw joint positions instead of hand-crafted\\nfeatures. We validate our system with a real-world infant movement dataset. Our\\nproposed channel attention module enables the visualization of the vital joints\\nto this disease that the network considers. Our system achieves 91.67%\\naccuracy, suppressing other state-of-the-art deep learning methods.\",\n",
       "        b\"The last several years have seen significant progress in using depth cameras\\nfor tracking articulated objects such as human bodies, hands, and robotic\\nmanipulators. Most approaches focus on tracking skeletal parameters of a fixed\\nshape model, which makes them insufficient for applications that require\\naccurate estimates of deformable object surfaces. To overcome this limitation,\\nwe present a 3D model-based tracking system for articulated deformable objects.\\nOur system is able to track human body pose and high resolution surface\\ncontours in real time using a commodity depth sensor and GPU hardware. We\\nimplement this as a joint optimization over a skeleton to account for changes\\nin pose, and over the vertices of a high resolution mesh to track the subject's\\nshape. Through experimental results we show that we are able to capture dynamic\\nsub-centimeter surface detail such as folds and wrinkles in clothing. We also\\nshow that this shape estimation aids kinematic pose estimation by providing a\\nmore accurate target to match against the point cloud. The end result is highly\\naccurate spatiotemporal and semantic information which is well suited for\\nphysical human robot interaction as well as virtual and augmented reality\\nsystems.\",\n",
       "        b'The identification and modeling of the terrain from point cloud data is an\\nimportant component of Terrestrial Remote Sensing (TRS) applications. The main\\nfocus in terrain modeling is capturing details of complex geological features\\nof landforms. Traditional terrain modeling approaches rely on the user to exert\\ncontrol over terrain features. However, relying on the user input to manually\\ndevelop the digital terrain becomes intractable when considering the amount of\\ndata generated by new remote sensing systems capable of producing massive\\naerial and ground-based point clouds from scanned environments. This article\\nprovides a novel terrain modeling technique capable of automatically generating\\naccurate and physically realistic Digital Terrain Models (DTM) from a variety\\nof point cloud data. The proposed method runs efficiently on large-scale point\\ncloud data with real-time performance over large segments of terrestrial\\nlandforms. Moreover, generated digital models are designed to effectively\\nrender within a Virtual Reality (VR) environment in real time. The paper\\nconcludes with an in-depth discussion of possible research directions and\\noutstanding technical and scientific challenges to improve the proposed\\napproach.',\n",
       "        b'Extracting moving objects from a video sequence and estimating the background\\nof each individual image are fundamental issues in many practical applications\\nsuch as visual surveillance, intelligent vehicle navigation, and traffic\\nmonitoring. Recently, some methods have been proposed to detect moving objects\\nin a video via low-rank approximation and sparse outliers where the background\\nis modeled with the computed low-rank component of the video and the foreground\\nobjects are detected as the sparse outliers in the low-rank approximation. All\\nof these existing methods work in a batch manner, preventing them from being\\napplied in real time and long duration tasks. In this paper, we present an\\nonline sequential framework, namely contiguous outliers representation via\\nonline low-rank approximation (COROLA), to detect moving objects and learn the\\nbackground model at the same time. We also show that our model can detect\\nmoving objects with a moving camera. Our experimental evaluation uses simulated\\ndata and real public datasets and demonstrates the superior performance of\\nCOROLA in terms of both accuracy and execution time.',\n",
       "        b'Recent blind super-resolution (SR) methods typically consist of two branches,\\none for degradation prediction and the other for conditional restoration.\\nHowever, our experiments show that a one-branch network can achieve comparable\\nperformance to the two-branch scheme. Then we wonder: how can one-branch\\nnetworks automatically learn to distinguish degradations? To find the answer,\\nwe propose a new diagnostic tool -- Filter Attribution method based on Integral\\nGradient (FAIG). Unlike previous integral gradient methods, our FAIG aims at\\nfinding the most discriminative filters instead of input pixels/features for\\ndegradation removal in blind SR networks. With the discovered filters, we\\nfurther develop a simple yet effective method to predict the degradation of an\\ninput image. Based on FAIG, we show that, in one-branch blind SR networks, 1)\\nwe are able to find a very small number of (1%) discriminative filters for each\\nspecific degradation; 2) The weights, locations and connections of the\\ndiscovered filters are all important to determine the specific network\\nfunction. 3) The task of degradation prediction can be implicitly realized by\\nthese discriminative filters without explicit supervised learning. Our findings\\ncan not only help us better understand network behaviors inside one-branch\\nblind SR networks, but also provide guidance on designing more efficient\\narchitectures and diagnosing networks for blind SR.',\n",
       "        b'Despite the remarkable success of deep learning, optimal convolution\\noperation on point cloud remains indefinite due to its irregular data\\nstructure. In this paper, we present Cubic Kernel Convolution (CKConv) that\\nlearns to voxelize the features of local points by exploiting both continuous\\nand discrete convolutions. Our continuous convolution uniquely employs a 3D\\ncubic form of kernel weight representation that splits a feature into voxels in\\nembedding space. By consecutively applying discrete 3D convolutions on the\\nvoxelized features in a spatial manner, preceding continuous convolution is\\nforced to learn spatial feature mapping, i.e., feature voxelization. In this\\nway, geometric information can be detailed by encoding with subdivided\\nfeatures, and our 3D convolutions on these fixed structured data do not suffer\\nfrom discretization artifacts thanks to voxelization in embedding space.\\nFurthermore, we propose a spatial attention module, Local Set Attention (LSA),\\nto provide comprehensive structure awareness within the local point set and\\nhence produce representative features. By learning feature voxelization with\\nLSA, CKConv can extract enriched features for effective point cloud analysis.\\nWe show that CKConv has great applicability to point cloud processing tasks\\nincluding object classification, object part segmentation, and scene semantic\\nsegmentation with state-of-the-art results.',\n",
       "        b'Attention mechanism has been shown to be effective for person\\nre-identification (Re-ID). However, the learned attentive feature embeddings\\nwhich are often not naturally diverse nor uncorrelated, will compromise the\\nretrieval performance based on the Euclidean distance. We advocate that\\nenforcing diversity could greatly complement the power of attention. To this\\nend, we propose an Attentive but Diverse Network (ABD-Net), which seamlessly\\nintegrates attention modules and diversity regularization throughout the entire\\nnetwork, to learn features that are representative, robust, and more\\ndiscriminative. Specifically, we introduce a pair of complementary attention\\nmodules, focusing on channel aggregation and position awareness, respectively.\\nFurthermore, a new efficient form of orthogonality constraint is derived to\\nenforce orthogonality on both hidden activations and weights. Through careful\\nablation studies, we verify that the proposed attentive and diverse terms each\\ncontributes to the performance gains of ABD-Net. On three popular benchmarks,\\nABD-Net consistently outperforms existing state-of-the-art methods.',\n",
       "        b'Recent research has witnessed the advances in facial image editing tasks. For\\nvideo editing, however, previous methods either simply apply transformations\\nframe by frame or utilize multiple frames in a concatenated or iterative\\nfashion, which leads to noticeable visual flickers. In addition, these methods\\nare confined to dealing with one specific task at a time without any\\nextensibility. In this paper, we propose a task-agnostic temporally consistent\\nfacial video editing framework. Based on a 3D reconstruction model, our\\nframework is designed to handle several editing tasks in a more unified and\\ndisentangled manner. The core design includes a dynamic training sample\\nselection mechanism and a novel 3D temporal loss constraint that fully exploits\\nboth image and video datasets and enforces temporal consistency. Compared with\\nthe state-of-the-art facial image editing methods, our framework generates\\nvideo portraits that are more photo-realistic and temporally smooth.',\n",
       "        b'Integration of reinforcement learning and imitation learning is an important\\nproblem that has been studied for a long time in the field of intelligent\\nrobotics. Reinforcement learning optimizes policies to maximize the cumulative\\nreward, whereas imitation learning attempts to extract general knowledge about\\nthe trajectories demonstrated by experts, i.e., demonstrators. Because each of\\nthem has their own drawbacks, methods combining them and compensating for each\\nset of drawbacks have been explored thus far. However, many of the methods are\\nheuristic and do not have a solid theoretical basis. In this paper, we present\\na new theory for integrating reinforcement and imitation learning by extending\\nthe probabilistic generative model framework for reinforcement learning, {\\\\it\\nplan by inference}. We develop a new probabilistic graphical model for\\nreinforcement learning with multiple types of rewards and a probabilistic\\ngraphical model for Markov decision processes with multiple optimality\\nemissions (pMDP-MO). Furthermore, we demonstrate that the integrated learning\\nmethod of reinforcement learning and imitation learning can be formulated as a\\nprobabilistic inference of policies on pMDP-MO by considering the output of the\\ndiscriminator in generative adversarial imitation learning as an additional\\noptimal emission observation. We adapt the generative adversarial imitation\\nlearning and task-achievement reward to our proposed framework, achieving\\nsignificantly better performance than agents trained with reinforcement\\nlearning or imitation learning alone. Experiments demonstrate that our\\nframework successfully integrates imitation and reinforcement learning even\\nwhen the number of demonstrators is only a few.',\n",
       "        b'Many recent successful (deep) reinforcement learning algorithms make use of\\nregularization, generally based on entropy or Kullback-Leibler divergence. We\\npropose a general theory of regularized Markov Decision Processes that\\ngeneralizes these approaches in two directions: we consider a larger class of\\nregularizers, and we consider the general modified policy iteration approach,\\nencompassing both policy iteration and value iteration. The core building\\nblocks of this theory are a notion of regularized Bellman operator and the\\nLegendre-Fenchel transform, a classical tool of convex optimization. This\\napproach allows for error propagation analyses of general algorithmic schemes\\nof which (possibly variants of) classical algorithms such as Trust Region\\nPolicy Optimization, Soft Q-learning, Stochastic Actor Critic or Dynamic Policy\\nProgramming are special cases. This also draws connections to proximal convex\\noptimization, especially to Mirror Descent.',\n",
       "        b'Digital libraries store images which can be highly degraded and to index this\\nkind of images we resort to word spot- ting as our information retrieval\\nsystem. Information retrieval for handwritten document images is more\\nchallenging due to the difficulties in complex layout analysis, large\\nvariations of writing styles, and degradation or low quality of historical\\nmanuscripts. This paper presents a simple innovative learning-free method for\\nword spotting from large scale historical documents combining Local Binary\\nPattern (LBP) and spatial sampling. This method offers three advantages:\\nfirstly, it operates in completely learning free paradigm which is very\\ndifferent from unsupervised learning methods, secondly, the computational time\\nis significantly low because of the LBP features which are very fast to\\ncompute, and thirdly, the method can be used in scenarios where annotations are\\nnot available. Finally we compare the results of our proposed retrieval method\\nwith the other methods in the literature.',\n",
       "        b'Recurrent neural network(RNN) has been broadly applied to natural language\\nprocessing(NLP) problems. This kind of neural network is designed for modeling\\nsequential data and has been testified to be quite efficient in sequential\\ntagging tasks. In this paper, we propose to use bi-directional RNN with long\\nshort-term memory(LSTM) units for Chinese word segmentation, which is a crucial\\npreprocess task for modeling Chinese sentences and articles. Classical methods\\nfocus on designing and combining hand-craft features from context, whereas\\nbi-directional LSTM network(BLSTM) does not need any prior knowledge or\\npre-designing, and it is expert in keeping the contextual information in both\\ndirections. Experiment result shows that our approach gets state-of-the-art\\nperformance in word segmentation on both traditional Chinese datasets and\\nsimplified Chinese datasets.',\n",
       "        b'We implement stacked denoising autoencoders, a class of neural networks that\\nare capable of learning powerful representations of high dimensional data. We\\ndescribe stochastic gradient descent for unsupervised training of autoencoders,\\nas well as a novel genetic algorithm based approach that makes use of gradient\\ninformation. We analyze the performance of both optimization algorithms and\\nalso the representation learning ability of the autoencoder when it is trained\\non standard image classification datasets.',\n",
       "        b'In this paper, we propose Object-driven Attentive Generative Adversarial\\nNewtorks (Obj-GANs) that allow object-centered text-to-image synthesis for\\ncomplex scenes. Following the two-step (layout-image) generation process, a\\nnovel object-driven attentive image generator is proposed to synthesize salient\\nobjects by paying attention to the most relevant words in the text description\\nand the pre-generated semantic layout. In addition, a new Fast R-CNN based\\nobject-wise discriminator is proposed to provide rich object-wise\\ndiscrimination signals on whether the synthesized object matches the text\\ndescription and the pre-generated layout. The proposed Obj-GAN significantly\\noutperforms the previous state of the art in various metrics on the large-scale\\nCOCO benchmark, increasing the Inception score by 27% and decreasing the FID\\nscore by 11%. A thorough comparison between the traditional grid attention and\\nthe new object-driven attention is provided through analyzing their mechanisms\\nand visualizing their attention layers, showing insights of how the proposed\\nmodel generates complex scenes in high quality.',\n",
       "        b'Taking the deep learning-based algorithms into account has become a crucial\\nway to boost object detection performance in aerial images. While various\\nneural network representations have been developed, previous works are still\\ninefficient to investigate the noise-resilient performance, especially on\\naerial images with noise taken by the cameras with telephoto lenses, and most\\nof the research is concentrated in the field of denoising. Of course, denoising\\nusually requires an additional computational burden to obtain higher quality\\nimages, while noise-resilient is more of a description of the robustness of the\\nnetwork itself to different noises, which is an attribute of the algorithm\\nitself. For this reason, the work will be started by analyzing the\\nnoise-resilient performance of the neural network, and then propose two\\nhypotheses to build a noise-resilient structure. Based on these hypotheses, we\\ncompare the noise-resilient ability of the Oct-ResNet with frequency division\\nprocessing and the commonly used ResNet. In addition, previous feature pyramid\\nnetworks used for aerial object detection tasks are not specifically designed\\nfor the frequency division feature maps of the Oct-ResNet, and they usually\\nlack attention to bridging the semantic gap between diverse feature maps from\\ndifferent depths. On the basis of this, a novel octave convolution-based\\nsemantic attention feature pyramid network (OcSaFPN) is proposed to get higher\\naccuracy in object detection with noise. The proposed algorithm tested on three\\ndatasets demonstrates that the proposed OcSaFPN achieves a state-of-the-art\\ndetection performance with Gaussian noise or multiplicative noise. In addition,\\nmore experiments have proved that the OcSaFPN structure can be easily added to\\nexisting algorithms, and the noise-resilient ability can be effectively\\nimproved.',\n",
       "        b'Deep Neural Networks (DNNs) have become ubiquitous in medical image\\nprocessing and analysis. Among them, U-Nets are very popular in various image\\nsegmentation tasks. Yet, little is known about how information flows through\\nthese networks and whether they are indeed properly designed for the tasks they\\nare being proposed for. In this paper, we employ information-theoretic tools in\\norder to gain insight into information flow through U-Nets. In particular, we\\nshow how mutual information between input/output and an intermediate layer can\\nbe a useful tool to understand information flow through various portions of a\\nU-Net, assess its architectural efficiency, and even propose more efficient\\ndesigns.',\n",
       "        b'Dynamic networks have shown their promising capability in reducing\\ntheoretical computation complexity by adapting their architectures to the input\\nduring inference. However, their practical runtime usually lags behind the\\ntheoretical acceleration due to inefficient sparsity. Here, we explore a\\nhardware-efficient dynamic inference regime, named dynamic weight slicing,\\nwhich adaptively slice a part of network parameters for inputs with diverse\\ndifficulty levels, while keeping parameters stored statically and contiguously\\nin hardware to prevent the extra burden of sparse computation. Based on this\\nscheme, we present dynamic slimmable network (DS-Net) and dynamic slice-able\\nnetwork (DS-Net++) by input-dependently adjusting filter numbers of CNNs and\\nmultiple dimensions in both CNNs and transformers, respectively. To ensure\\nsub-network generality and routing fairness, we propose a disentangled\\ntwo-stage optimization scheme with training techniques such as in-place\\nbootstrapping (IB), multi-view consistency (MvCo) and sandwich gate\\nsparsification (SGS) to train supernet and gate separately. Extensive\\nexperiments on 4 datasets and 3 different network architectures demonstrate our\\nmethod consistently outperforms state-of-the-art static and dynamic model\\ncompression methods by a large margin (up to 6.6%). Typically, DS-Net++\\nachieves 2-4x computation reduction and 1.62x real-world acceleration over\\nMobileNet, ResNet-50 and Vision Transformer, with minimal accuracy drops\\n(0.1-0.3%) on ImageNet. Code release: https://github.com/changlin31/DS-Net',\n",
       "        b'We propose a method to refine geometry of 3D meshes from a consumer level\\ndepth camera, e.g. Kinect, by exploiting shading cues captured from an infrared\\n(IR) camera. A major benefit to using an IR camera instead of an RGB camera is\\nthat the IR images captured are narrow band images that filter out most\\nundesired ambient light, which makes our system robust against natural indoor\\nillumination. Moreover, for many natural objects with colorful textures in the\\nvisible spectrum, the subjects appear to have a uniform albedo in the IR\\nspectrum. Based on our analyses on the IR projector light of the Kinect, we\\ndefine a near light source IR shading model that describes the captured\\nintensity as a function of surface normals, albedo, lighting direction, and\\ndistance between light source and surface points. To resolve the ambiguity in\\nour model between the normals and distances, we utilize an initial 3D mesh from\\nthe Kinect fusion and multi-view information to reliably estimate surface\\ndetails that were not captured and reconstructed by the Kinect fusion. Our\\napproach directly operates on the mesh model for geometry refinement. We ran\\nexperiments on our algorithm for geometries captured by both the Kinect I and\\nKinect II, as the depth acquisition in Kinect I is based on a structured-light\\ntechnique and that of the Kinect II is based on a time-of-flight (ToF)\\ntechnology. The effectiveness of our approach is demonstrated through several\\nchallenging real-world examples. We have also performed a user study to\\nevaluate the quality of the mesh models before and after our refinements.',\n",
       "        b'We propose a practical framework to address the problem of privacy-aware\\nimage sharing in large-scale setups. We argue that, while compactness is always\\ndesired at scale, this need is more severe when trying to furthermore protect\\nthe privacy-sensitive content. We therefore encode images, such that, from one\\nhand, representations are stored in the public domain without paying the huge\\ncost of privacy protection, but ambiguated and hence leaking no discernible\\ncontent from the images, unless a combinatorially-expensive guessing mechanism\\nis available for the attacker. From the other hand, authorized users are\\nprovided with very compact keys that can easily be kept secure. This can be\\nused to disambiguate and reconstruct faithfully the corresponding\\naccess-granted images. We achieve this with a convolutional autoencoder of our\\ndesign, where feature maps are passed independently through sparsifying\\ntransformations, providing multiple compact codes, each responsible for\\nreconstructing different attributes of the image. The framework is tested on a\\nlarge-scale database of images with public implementation available.',\n",
       "        b'Point cloud-based large scale place recognition is fundamental for many\\napplications like Simultaneous Localization and Mapping (SLAM). Although many\\nmodels have been proposed and have achieved good performance by learning\\nshort-range local features, long-range contextual properties have often been\\nneglected. Moreover, the model size has also become a bottleneck for their wide\\napplications. To overcome these challenges, we propose a super light-weight\\nnetwork model termed SVT-Net for large scale place recognition. Specifically,\\non top of the highly efficient 3D Sparse Convolution (SP-Conv), an Atom-based\\nSparse Voxel Transformer (ASVT) and a Cluster-based Sparse Voxel Transformer\\n(CSVT) are proposed to learn both short-range local features and long-range\\ncontextual features in this model. Consisting of ASVT and CSVT, SVT-Net can\\nachieve state-of-the-art on benchmark datasets in terms of both accuracy and\\nspeed with a super-light model size (0.9M). Meanwhile, two simplified versions\\nof SVT-Net are introduced, which also achieve state-of-the-art and further\\nreduce the model size to 0.8M and 0.4M respectively.',\n",
       "        b\"This paper presents a new approach for synthesizing a novel street-view\\npanorama given an overhead satellite image. Taking a small satellite image\\npatch as input, our method generates a Google's omnidirectional street-view\\ntype panorama, as if it is captured from the same geographical location as the\\ncenter of the satellite patch. Existing works tackle this task as an image\\ngeneration problem which adopts generative adversarial networks to implicitly\\nlearn the cross-view transformations, while ignoring the domain relevance. In\\nthis paper, we propose to explicitly establish the geometric correspondences\\nbetween the two-view images so as to facilitate the cross-view transformation\\nlearning. Specifically, we observe that when a 3D point in the real world is\\nvisible in both views, there is a deterministic mapping between the projected\\npoints in the two-view images given the height information of this 3D point.\\nMotivated by this, we develop a novel Satellite to Street-view image Projection\\n(S2SP) module which explicitly establishes such geometric correspondences and\\nprojects the satellite images to the street viewpoint. With these projected\\nsatellite images as network input, we next employ a generator to synthesize\\nrealistic street-view panoramas that are geometrically consistent with the\\nsatellite images. Our S2SP module is differentiable and the whole framework is\\ntrained in an end-to-end manner. Extensive experimental results on two\\ncross-view benchmark datasets demonstrate that our method generates images that\\nbetter respect the scene geometry than existing approaches.\",\n",
       "        b'Level set methods are widely used for image segmentation because of their\\ncapability to handle topological changes. In this paper, we propose a novel\\nparametric level set method called Disjunctive Normal Level Set (DNLS), and\\napply it to both two phase (single object) and multiphase (multi-object) image\\nsegmentations. The DNLS is formed by union of polytopes which themselves are\\nformed by intersections of half-spaces. The proposed level set framework has\\nthe following major advantages compared to other level set methods available in\\nthe literature. First, segmentation using DNLS converges much faster. Second,\\nthe DNLS level set function remains regular throughout its evolution. Third,\\nthe proposed multiphase version of the DNLS is less sensitive to\\ninitialization, and its computational cost and memory requirement remains\\nalmost constant as the number of objects to be simultaneously segmented grows.\\nThe experimental results show the potential of the proposed method.',\n",
       "        b'A key aspect of machine learning models lies in their ability to learn\\nefficient intermediate features. However, the input representation plays a\\ncrucial role in this process, and polyphonic musical scores remain a\\nparticularly complex type of information. In this paper, we introduce a novel\\nrepresentation of symbolic music data, which transforms a polyphonic score into\\na continuous signal. We evaluate the ability to learn meaningful features from\\nthis representation from a musical point of view. Hence, we introduce an\\nevaluation method relying on principled generation of synthetic data. Finally,\\nto test our proposed representation we conduct an extensive benchmark against\\nrecent polyphonic symbolic representations. We show that our signal-like\\nrepresentation leads to better reconstruction and disentangled features. This\\nimprovement is reflected in the metric properties and in the generation ability\\nof the space learned from our signal-like representation according to music\\ntheory properties.',\n",
       "        b'Automatic extraction of road curbs from uneven, unorganized, noisy and\\nmassive 3D point clouds is a challenging task. Existing methods often project\\n3D point clouds onto 2D planes to extract curbs. However, the projection causes\\nloss of 3D information which degrades the performance of the detection. This\\npaper presents a robust, accurate and efficient method to extract road curbs\\nfrom 3D mobile LiDAR point clouds. Our method consists of two steps: 1)\\nextracting the candidate points of curbs based on the proposed novel energy\\nfunction and 2) refining the candidate points using the proposed least cost\\npath model. We evaluated our method on a large-scale of residential area\\n(16.7GB, 300 million points) and an urban area (1.07GB, 20 million points)\\nmobile LiDAR point clouds. Results indicate that the proposed method is\\nsuperior to the state-of-the-art methods in terms of robustness, accuracy and\\nefficiency. The proposed curb extraction method achieved a completeness of\\n78.62% and a correctness of 83.29%. These experiments demonstrate that the\\nproposed method is a promising solution to extract road curbs from mobile LiDAR\\npoint clouds.',\n",
       "        b'Combining off-policy reinforcement learning methods with function\\napproximators such as neural networks has been found to lead to overestimation\\nof the value function and sub-optimal solutions. Improvement such as TD3 has\\nbeen proposed to address this issue. However, we surprisingly find that its\\nperformance lags behind the vanilla actor-critic methods (such as DDPG) in some\\nprimitive environments. In this paper, we show that the failure of some cases\\ncan be attributed to insufficient exploration. We reveal the culprit of\\ninsufficient exploration in TD3, and propose a novel algorithm toward this\\nproblem that ADapts between Exploration and Robustness, namely ADER. To enhance\\nthe exploration ability while eliminating the overestimation bias, we introduce\\na dynamic penalty term in value estimation calculated from estimated\\nuncertainty, which takes into account different compositions of the uncertainty\\nin different learning stages. Experiments in several challenging environments\\ndemonstrate the supremacy of the proposed method in continuous control tasks.',\n",
       "        b'Process analytics is an umbrella of data-driven techniques which includes\\nmaking predictions for individual process instances or overall process models.\\nAt the instance level, various novel techniques have been recently devised,\\ntackling next activity, remaining time, and outcome prediction. At the model\\nlevel, there is a notable void. It is the ambition of this paper to fill this\\ngap. To this end, we develop a technique to forecast the entire process model\\nfrom historical event data. A forecasted model is a will-be process model\\nrepresenting a probable future state of the overall process. Such a forecast\\nhelps to investigate the consequences of drift and emerging bottlenecks. Our\\ntechnique builds on a representation of event data as multiple time series,\\neach capturing the evolution of a behavioural aspect of the process model, such\\nthat corresponding forecasting techniques can be applied. Our implementation\\ndemonstrates the accuracy of our technique on real-world event log data.',\n",
       "        b'Due to the advances in computing and sensing, deep learning (DL) has widely\\nbeen applied in smart energy systems (SESs). These DL-based solutions have\\nproved their potentials in improving the effectiveness and adaptiveness of the\\ncontrol systems. However, in recent years, increasing evidence shows that DL\\ntechniques can be manipulated by adversarial attacks with carefully-crafted\\nperturbations. Adversarial attacks have been studied in computer vision and\\nnatural language processing. However, there is very limited work focusing on\\nthe adversarial attack deployment and mitigation in energy systems. In this\\nregard, to better prepare the SESs against potential adversarial attacks, we\\npropose an innovative adversarial attack model that can practically compromise\\ndynamical controls of energy system. We also optimize the deployment of the\\nproposed adversarial attack model by employing deep reinforcement learning (RL)\\ntechniques. In this paper, we present our first-stage work in this direction.\\nIn simulation section, we evaluate the performance of our proposed adversarial\\nattack model using standard IEEE 9-bus system.',\n",
       "        b'Computational results demonstrate that posterior sampling for reinforcement\\nlearning (PSRL) dramatically outperforms algorithms driven by optimism, such as\\nUCRL2. We provide insight into the extent of this performance boost and the\\nphenomenon that drives it. We leverage this insight to establish an\\n$\\\\tilde{O}(H\\\\sqrt{SAT})$ Bayesian expected regret bound for PSRL in\\nfinite-horizon episodic Markov decision processes, where $H$ is the horizon,\\n$S$ is the number of states, $A$ is the number of actions and $T$ is the time\\nelapsed. This improves upon the best previous bound of $\\\\tilde{O}(H S\\n\\\\sqrt{AT})$ for any reinforcement learning algorithm.',\n",
       "        b'Recent advances in document image analysis (DIA) have been primarily driven\\nby the application of neural networks. Ideally, research outcomes could be\\neasily deployed in production and extended for further investigation. However,\\nvarious factors like loosely organized codebases and sophisticated model\\nconfigurations complicate the easy reuse of important innovations by a wide\\naudience. Though there have been on-going efforts to improve reusability and\\nsimplify deep learning (DL) model development in disciplines like natural\\nlanguage processing and computer vision, none of them are optimized for\\nchallenges in the domain of DIA. This represents a major gap in the existing\\ntoolkit, as DIA is central to academic research across a wide range of\\ndisciplines in the social sciences and humanities. This paper introduces\\nlayoutparser, an open-source library for streamlining the usage of DL in DIA\\nresearch and applications. The core layoutparser library comes with a set of\\nsimple and intuitive interfaces for applying and customizing DL models for\\nlayout detection, character recognition, and many other document processing\\ntasks. To promote extensibility, layoutparser also incorporates a community\\nplatform for sharing both pre-trained models and full document digitization\\npipelines. We demonstrate that layoutparser is helpful for both lightweight and\\nlarge-scale digitization pipelines in real-word use cases. The library is\\npublicly available at https://layout-parser.github.io/.',\n",
       "        b'Shape instantiation which predicts the 3D shape of a dynamic target from one\\nor more 2D images is important for real-time intra-operative navigation.\\nPreviously, a general shape instantiation framework was proposed with manual\\nimage segmentation to generate a 2D Statistical Shape Model (SSM) and with\\nKernel Partial Least Square Regression (KPLSR) to learn the relationship\\nbetween the 2D and 3D SSM for 3D shape prediction. In this paper, the two-stage\\nshape instantiation is improved to be one-stage. PointOutNet with 19\\nconvolutional layers and three fully-connected layers is used as the network\\nstructure and Chamfer distance is used as the loss function to predict the 3D\\ntarget point cloud from a single 2D image. With the proposed one-stage shape\\ninstantiation algorithm, a spontaneous image-to-point cloud training and\\ninference can be achieved. A dataset from 27 Right Ventricle (RV) subjects,\\nindicating 609 experiments, were used to validate the proposed one-stage shape\\ninstantiation algorithm. An average point cloud-to-point cloud (PC-to-PC) error\\nof 1.72mm has been achieved, which is comparable to the PLSR-based (1.42mm) and\\nKPLSR-based (1.31mm) two-stage shape instantiation algorithm.',\n",
       "        b'We consider model selection in stochastic bandit and reinforcement learning\\nproblems. Given a set of base learning algorithms, an effective model selection\\nstrategy adapts to the best learning algorithm in an online fashion. We show\\nthat by estimating the regret of each algorithm and playing the algorithms such\\nthat all empirical regrets are ensured to be of the same order, the overall\\nregret balancing strategy achieves a regret that is close to the regret of the\\noptimal base algorithm. Our strategy requires an upper bound on the optimal\\nbase regret as input, and the performance of the strategy depends on the\\ntightness of the upper bound. We show that having this prior knowledge is\\nnecessary in order to achieve a near-optimal regret. Further, we show that any\\nnear-optimal model selection strategy implicitly performs a form of regret\\nbalancing.',\n",
       "        b'Components of biological systems interact with each other in order to carry\\nout vital cell functions. Such information can be used to improve estimation\\nand inference, and to obtain better insights into the underlying cellular\\nmechanisms. Discovering regulatory interactions among genes is therefore an\\nimportant problem in systems biology. Whole-genome expression data over time\\nprovides an opportunity to determine how the expression levels of genes are\\naffected by changes in transcription levels of other genes, and can therefore\\nbe used to discover regulatory interactions among genes.\\n  In this paper, we propose a novel penalization method, called truncating\\nlasso, for estimation of causal relationships from time-course gene expression\\ndata. The proposed penalty can correctly determine the order of the underlying\\ntime series, and improves the performance of the lasso-type estimators.\\nMoreover, the resulting estimate provides information on the time lag between\\nactivation of transcription factors and their effects on regulated genes. We\\nprovide an efficient algorithm for estimation of model parameters, and show\\nthat the proposed method can consistently discover causal relationships in the\\nlarge $p$, small $n$ setting. The performance of the proposed model is\\nevaluated favorably in simulated, as well as real, data examples. The proposed\\ntruncating lasso method is implemented in the R-package grangerTlasso and is\\navailable at http://www.stat.lsa.umich.edu/~shojaie.',\n",
       "        b'In this paper, we extend the traditional few-shot learning (FSL) problem to\\nthe situation when the source-domain data is not accessible but only high-level\\ninformation in the form of class prototypes is available. This limited\\ninformation setup for the FSL problem deserves much attention due to its\\nimplication of privacy-preserving inaccessibility to the source-domain data but\\nit has rarely been addressed before. Because of limited training data, we\\npropose a non-parametric approach to this FSL problem by assuming that all the\\nclass prototypes are structurally arranged on a manifold. Accordingly, we\\nestimate the novel-class prototype locations by projecting the few-shot samples\\nonto the average of the subspaces on which the surrounding classes lie. During\\nclassification, we again exploit the structural arrangement of the categories\\nby inducing a Markov chain on the graph constructed with the class prototypes.\\nThis manifold distance obtained using the Markov chain is expected to produce\\nbetter results compared to a traditional nearest-neighbor-based Euclidean\\ndistance. To evaluate our proposed framework, we have tested it on two image\\ndatasets - the large-scale ImageNet and the small-scale but fine-grained\\nCUB-200. We have also studied parameter sensitivity to better understand our\\nframework.',\n",
       "        b'The field of Explainable Artificial Intelligence (XAI) aims to build\\nexplainable and interpretable machine learning (or deep learning) methods\\nwithout sacrificing prediction performance. Convolutional Neural Networks\\n(CNNs) have been successful in making predictions, especially in image\\nclassification. However, these famous deep learning models use tens of millions\\nof parameters based on a large number of pre-trained filters which have been\\nrepurposed from previous data sets. We propose a novel Interaction-based\\nConvolutional Neural Network (ICNN) that does not make assumptions about the\\nrelevance of local information. Instead, we use a model-free Influence Score\\n(I-score) to directly extract the influential information from images to form\\nimportant variable modules. We demonstrate that the proposed method produces\\nstate-of-the-art prediction performance of 99.8% on a real-world data set\\nclassifying COVID-19 Chest X-ray images without sacrificing the explanatory\\npower of the model. This proposed design can efficiently screen COVID-19\\npatients before human diagnosis, and will be the benchmark for addressing\\nfuture XAI problems in large-scale data sets.',\n",
       "        b'Reinforcement learning agents often forget details of the past, especially\\nafter delays or distractor tasks. Agents with common memory architectures\\nstruggle to recall and integrate across multiple timesteps of a past event, or\\neven to recall the details of a single timestep that is followed by distractor\\ntasks. To address these limitations, we propose a Hierarchical Transformer\\nMemory (HTM), which helps agents to remember the past in detail. HTM stores\\nmemories by dividing the past into chunks, and recalls by first performing\\nhigh-level attention over coarse summaries of the chunks, and then performing\\ndetailed attention within only the most relevant chunks. An agent with HTM can\\ntherefore \"mentally time-travel\" -- remember past events in detail without\\nattending to all intervening events. We show that agents with HTM substantially\\noutperform agents with other memory architectures at tasks requiring long-term\\nrecall, retention, or reasoning over memory. These include recalling where an\\nobject is hidden in a 3D environment, rapidly learning to navigate efficiently\\nin a new neighborhood, and rapidly learning and retaining new object names.\\nAgents with HTM can extrapolate to task sequences an order of magnitude longer\\nthan they were trained on, and can even generalize zero-shot from a\\nmeta-learning setting to maintaining knowledge across episodes. HTM improves\\nagent sample efficiency, generalization, and generality (by solving tasks that\\npreviously required specialized architectures). Our work is a step towards\\nagents that can learn, interact, and adapt in complex and temporally-extended\\nenvironments.',\n",
       "        b'HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their\\npotential in modeling high-order relations preserved in graph structured data.\\nHowever, most existing convolution filters are localized and determined by the\\npre-defined initial hypergraph topology, neglecting to explore implicit and\\nlong-ange relations in real-world data. In this paper, we propose the first\\nlearning-based method tailored for constructing adaptive hypergraph structure,\\ntermed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic\\nplug-in-play module for improving the representational power of HGCNNs.\\nSpecifically, HERALD adaptively optimizes the adjacency relationship between\\nhypernodes and hyperedges in an end-to-end manner and thus the task-aware\\nhypergraph is learned. Furthermore, HERALD employs the self-attention mechanism\\nto capture the non-local paired-nodes relation. Extensive experiments on\\nvarious popular hypergraph datasets for node classification and graph\\nclassification tasks demonstrate that our approach obtains consistent and\\nconsiderable performance enhancement, proving its effectiveness and\\ngeneralization ability.',\n",
       "        b'Smartphones, wearables, and Internet of Things (IoT) devices produce a wealth\\nof data that cannot be accumulated in a centralized repository for learning\\nsupervised models due to privacy, bandwidth limitations, and the prohibitive\\ncost of annotations. Federated learning provides a compelling framework for\\nlearning models from decentralized data, but conventionally, it assumes the\\navailability of labeled samples, whereas on-device data are generally either\\nunlabeled or cannot be annotated readily through user interaction. To address\\nthese issues, we propose a self-supervised approach termed\\n\\\\textit{scalogram-signal correspondence learning} based on wavelet transform to\\nlearn useful representations from unlabeled sensor inputs, such as\\nelectroencephalography, blood volume pulse, accelerometer, and WiFi channel\\nstate information. Our auxiliary task requires a deep temporal neural network\\nto determine if a given pair of a signal and its complementary viewpoint (i.e.,\\na scalogram generated with a wavelet transform) align with each other or not\\nthrough optimizing a contrastive objective. We extensively assess the quality\\nof learned features with our multi-view strategy on diverse public datasets,\\nachieving strong performance in all domains. We demonstrate the effectiveness\\nof representations learned from an unlabeled input collection on downstream\\ntasks with training a linear classifier over pretrained network, usefulness in\\nlow-data regime, transfer learning, and cross-validation. Our methodology\\nachieves competitive performance with fully-supervised networks, and it\\noutperforms pre-training with autoencoders in both central and federated\\ncontexts. Notably, it improves the generalization in a semi-supervised setting\\nas it reduces the volume of labeled data required through leveraging\\nself-supervised learning.',\n",
       "        b'Convolutional Neural Networks (CNN) has been widely applied in the realm of\\ncomputer vision. However, given the fact that CNN models are translation\\ninvariant, they are not aware of the coordinate information of each pixel. Thus\\nthe generalization ability of CNN will be limited since the coordinate\\ninformation is crucial for a model to learn affine transformations which\\ndirectly operate on the coordinate of each pixel. In this project, we proposed\\na simple approach to incorporate the coordinate information to the CNN model\\nthrough coordinate embedding. Our approach does not change the downstream model\\narchitecture and can be easily applied to the pre-trained models for the task\\nlike object detection. Our experiments on the German Traffic Sign Detection\\nBenchmark show that our approach not only significantly improve the model\\nperformance but also have better robustness with respect to the affine\\ntransformation.',\n",
       "        b'Greyscale image colorization for applications in image restoration has seen\\nsignificant improvements in recent years. Many of these techniques that use\\nlearning-based methods struggle to effectively colorize sparse inputs. With the\\nconsistent growth of the anime industry, the ability to colorize sparse input\\nsuch as line art can reduce significant cost and redundant work for production\\nstudios by eliminating the in-between frame colorization process. Simply using\\nexisting methods yields inconsistent colors between related frames resulting in\\na flicker effect in the final video. In order to successfully automate key\\nareas of large-scale anime production, the colorization of line arts must be\\ntemporally consistent between frames. This paper proposes a method to colorize\\nline art frames in an adversarial setting, to create temporally coherent video\\nof large anime by improving existing image to image translation methods. We\\nshow that by adding an extra condition to the generator and discriminator, we\\ncan effectively create temporally consistent video sequences from anime line\\narts. Code and models available at: https://github.com/Harry-Thasarathan/TCVC',\n",
       "        b'Machine learning (ML) models that learn and predict properties of computer\\nprograms are increasingly being adopted and deployed. These models have\\ndemonstrated success in applications such as auto-completing code, summarizing\\nlarge programs, and detecting bugs and malware in programs. In this work, we\\ninvestigate principled ways to adversarially perturb a computer program to fool\\nsuch learned models, and thus determine their adversarial robustness. We use\\nprogram obfuscations, which have conventionally been used to avoid attempts at\\nreverse engineering programs, as adversarial perturbations. These perturbations\\nmodify programs in ways that do not alter their functionality but can be\\ncrafted to deceive an ML model when making a decision. We provide a general\\nformulation for an adversarial program that allows applying multiple\\nobfuscation transformations to a program in any language. We develop\\nfirst-order optimization algorithms to efficiently determine two key aspects --\\nwhich parts of the program to transform, and what transformations to use. We\\nshow that it is important to optimize both these aspects to generate the best\\nadversarially perturbed program. Due to the discrete nature of this problem, we\\nalso propose using randomized smoothing to improve the attack loss landscape to\\nease optimization. We evaluate our work on Python and Java programs on the\\nproblem of program summarization. We show that our best attack proposal\\nachieves a $52\\\\%$ improvement over a state-of-the-art attack generation\\napproach for programs trained on a seq2seq model. We further show that our\\nformulation is better at training models that are robust to adversarial\\nattacks.',\n",
       "        b'Many semantic events in team sport activities e.g. basketball often involve\\nboth group activities and the outcome (score or not). Motion patterns can be an\\neffective means to identify different activities. Global and local motions have\\ntheir respective emphasis on different activities, which are difficult to\\ncapture from the optical flow due to the mixture of global and local motions.\\nHence it calls for a more effective way to separate the global and local\\nmotions. When it comes to the specific case for basketball game analysis, the\\nsuccessful score for each round can be reliably detected by the appearance\\nvariation around the basket. Based on the observations, we propose a scheme to\\nfuse global and local motion patterns (MPs) and key visual information (KVI)\\nfor semantic event recognition in basketball videos. Firstly, an algorithm is\\nproposed to estimate the global motions from the mixed motions based on the\\nintrinsic property of camera adjustments. And the local motions could be\\nobtained from the mixed and global motions. Secondly, a two-stream 3D CNN\\nframework is utilized for group activity recognition over the separated global\\nand local motion patterns. Thirdly, the basket is detected and its appearance\\nfeatures are extracted through a CNN structure. The features are utilized to\\npredict the success or failure. Finally, the group activity recognition and\\nsuccess/failure prediction results are integrated using the kronecker product\\nfor event recognition. Experiments on NCAA dataset demonstrate that the\\nproposed method obtains state-of-the-art performance.',\n",
       "        b'Building a good predictive model requires an array of activities such as data\\nimputation, feature transformations, estimator selection, hyper-parameter\\nsearch and ensemble construction. Given the large, complex and heterogenous\\nspace of options, off-the-shelf optimization methods are infeasible for\\nrealistic response times. In practice, much of the predictive modeling process\\nis conducted by experienced data scientists, who selectively make use of\\navailable tools. Over time, they develop an understanding of the behavior of\\noperators, and perform serial decision making under uncertainty, colloquially\\nreferred to as educated guesswork. With an unprecedented demand for application\\nof supervised machine learning, there is a call for solutions that\\nautomatically search for a good combination of parameters across these tasks to\\nminimize the modeling error. We introduce a novel system called APRL\\n(Autonomous Predictive modeler via Reinforcement Learning), that uses past\\nexperience through reinforcement learning to optimize such sequential decision\\nmaking from within a set of diverse actions under a time constraint on a\\npreviously unseen predictive learning problem. APRL actions are taken to\\noptimize the performance of a final ensemble. This is in contrast to other\\nsystems, which maximize individual model accuracy first and create ensembles as\\na disconnected post-processing step. As a result, APRL is able to reduce up to\\n71\\\\% of classification error on average over a wide variety of problems.',\n",
       "        b'Robust Anomaly Detection (AD) on time series data is a key component for\\nmonitoring many complex modern systems. These systems typically generate\\nhigh-dimensional time series that can be highly noisy, seasonal, and\\ninter-correlated. This paper explores some of the challenges in such data, and\\nproposes a new approach that makes inroads towards increased robustness on\\nseasonal and contaminated data, while providing a better root cause\\nidentification of anomalies. In particular, we propose the use of Robust\\nSeasonal Multivariate Generative Adversarial Network (RSM-GAN) that extends\\nrecent advancements in GAN with the adoption of convolutional-LSTM layers and\\nattention mechanisms to produce excellent performance on various settings. We\\nconduct extensive experiments in which not only do this model displays more\\nrobust behavior on complex seasonality patterns, but also shows increased\\nresistance to training data contamination. We compare it with existing\\nclassical and deep-learning AD models, and show that this architecture is\\nassociated with the lowest false positive rate and improves precision by 30%\\nand 16% in real-world and synthetic data, respectively.',\n",
       "        b\"Many cultures around the world believe that palm reading can be used to\\npredict the future life of a person. Palmistry uses features of the hand such\\nas palm lines, hand shape, or fingertip position. However, the research on\\npalm-line detection is still scarce, many of them applied traditional image\\nprocessing techniques. In most real-world scenarios, images usually are not in\\nwell-conditioned, causing these methods to severely under-perform. In this\\npaper, we propose an algorithm to extract principle palm lines from an image of\\na person's hand. Our method applies deep learning networks (DNNs) to improve\\nperformance. Another challenge of this problem is the lack of training data. To\\ndeal with this issue, we handcrafted a dataset from scratch. From this dataset,\\nwe compare the performance of readily available methods with ours. Furthermore,\\nbased on the UNet segmentation neural network architecture and the knowledge of\\nattention mechanism, we propose a highly efficient architecture to detect\\npalm-lines. We proposed the Context Fusion Module to capture the most important\\ncontext feature, which aims to improve segmentation accuracy. The experimental\\nresults show that it outperforms the other methods with the highest F1 Score\\nabout 99.42% and mIoU is 0.584 for the same dataset.\",\n",
       "        b'Artificial neural networks use a lot of coefficients that take a great deal\\nof computing power for their adjustment, especially if deep learning networks\\nare employed. However, there exist coefficients-free extremely fast\\nindexing-based technologies that work, for instance, in Google search engines,\\nin genome sequencing, etc. The paper discusses the use of indexing-based\\nmethods for pattern recognition. It is shown that for pattern recognition\\napplications such indexing methods replace with inverse patterns the fully\\ninverted files, which are typically employed in search engines. Not only such\\ninversion provide automatic feature extraction, which is a distinguishing mark\\nof deep learning, but, unlike deep learning, pattern inversion supports almost\\ninstantaneous learning, which is a consequence of absence of coefficients. The\\npaper discusses a pattern inversion formalism that makes use on a novel pattern\\ntransform and its application for unsupervised instant learning. Examples\\ndemonstrate a view-angle independent recognition of three-dimensional objects,\\nsuch as cars, against arbitrary background, prediction of remaining useful life\\nof aircraft engines, and other applications. In conclusion, it is noted that,\\nin neurophysiology, the function of the neocortical mini-column has been widely\\ndebated since 1957. This paper hypothesize that, mathematically, the cortical\\nmini-column can be described as an inverse pattern, which physically serves as\\na connection multiplier expanding associations of inputs with relevant pattern\\nclasses.',\n",
       "        b'An efficient spatial regularization method using superpixel segmentation and\\ngraph Laplacian regularization is proposed for sparse hyperspectral unmixing\\nmethod. Since it is likely to find spectrally similar pixels in a homogeneous\\nregion, we use a superpixel segmentation algorithm to extract the homogeneous\\nregions by considering the image boundaries. We first extract the homogeneous\\nregions, which are called superpixels, then a weighted graph in each superpixel\\nis constructed by selecting $K$-nearest pixels in each superpixel. Each node in\\nthe graph represents the spectrum of a pixel and edges connect the similar\\npixels inside the superpixel. The spatial similarity is investigated using\\ngraph Laplacian regularization. Sparsity regularization for abundance matrix is\\nprovided using a weighted sparsity promoting norm. Experimental results on\\nsimulated and real data sets show the superiority of the proposed algorithm\\nover the well-known algorithms in the literature.',\n",
       "        b'We propose a framework for aligning and fusing multiple images into a single\\ncoordinate-based neural representations. Our framework targets burst images\\nthat have misalignment due to camera ego motion and small changes in the scene.\\nWe describe different strategies for alignment depending on the assumption of\\nthe scene motion, namely, perspective planar (i.e., homography), optical flow\\nwith minimal scene change, and optical flow with notable occlusion and\\ndisocclusion. Our framework effectively combines the multiple inputs into a\\nsingle neural implicit function without the need for selecting one of the\\nimages as a reference frame. We demonstrate how to use this multi-frame fusion\\nframework for various layer separation tasks.',\n",
       "        b'Production machine learning systems are consistently under attack by\\nadversarial actors. Various deep learning models must be capable of accurately\\ndetecting fake or adversarial input while maintaining speed. In this work, we\\npropose one piece of the production protection system: detecting an incoming\\nadversarial attack and its characteristics. Detecting types of adversarial\\nattacks has two primary effects: the underlying model can be trained in a\\nstructured manner to be robust from those attacks and the attacks can be\\npotentially filtered out in real-time before causing any downstream damage. The\\nadversarial image classification space is explored for models commonly used in\\ntransfer learning.',\n",
       "        b'We study multi-task reinforcement learning (RL) in tabular episodic Markov\\ndecision processes (MDPs). We formulate a heterogeneous multi-player RL\\nproblem, in which a group of players concurrently face similar but not\\nnecessarily identical MDPs, with a goal of improving their collective\\nperformance through inter-player information sharing. We design and analyze an\\nalgorithm based on the idea of model transfer, and provide gap-dependent and\\ngap-independent upper and lower bounds that characterize the intrinsic\\ncomplexity of the problem.',\n",
       "        b'This paper investigates the optimal signal detection problem with a\\nparticular interest in large-scale multiple-input multiple-output (MIMO)\\nsystems. The problem is NP-hard and can be solved optimally by searching the\\nshortest path on the decision tree. Unfortunately, the existing optimal search\\nalgorithms often involve prohibitively high complexities, which indicates that\\nthey are infeasible in large-scale MIMO systems. To address this issue, we\\npropose a general heuristic search algorithm, namely, hyperaccelerated tree\\nsearch (HATS) algorithm. The proposed algorithm employs a deep neural network\\n(DNN) to estimate the optimal heuristic, and then use the estimated heuristic\\nto speed up the underlying memory-bounded search algorithm. This idea is\\ninspired by the fact that the underlying heuristic search algorithm reaches the\\noptimal efficiency with the optimal heuristic function. Simulation results show\\nthat the proposed algorithm reaches almost the optimal bit error rate (BER)\\nperformance in large-scale systems, while the memory size can be bounded. In\\nthe meanwhile, it visits nearly the fewest tree nodes. This indicates that the\\nproposed algorithm reaches almost the optimal efficiency in practical\\nscenarios, and thereby it is applicable for large-scale systems. Besides, the\\ncode for this paper is available at https://github.com/skypitcher/hats.',\n",
       "        b'In the conventional person Re-ID setting, it is widely assumed that cropped\\nperson images are for each individual. However, in a crowded scene,\\noff-shelf-detectors may generate bounding boxes involving multiple people,\\nwhere the large proportion of background pedestrians or human occlusion exists.\\nThe representation extracted from such cropped images, which contain both the\\ntarget and the interference pedestrians, might include distractive information.\\nThis will lead to wrong retrieval results. To address this problem, this paper\\npresents a novel deep network termed Pedestrian-Interference Suppression\\nNetwork (PISNet). PISNet leverages a Query-Guided Attention Block (QGAB) to\\nenhance the feature of the target in the gallery, under the guidance of the\\nquery. Furthermore, the involving Guidance Reversed Attention Module and the\\nMulti-Person Separation Loss promote QGAB to suppress the interference of other\\npedestrians. Our method is evaluated on two new pedestrian-interference\\ndatasets and the results show that the proposed method performs favorably\\nagainst existing Re-ID methods.',\n",
       "        b'Safety is a critical feature of controller design for physical systems. When\\ndesigning control policies, several approaches to guarantee this aspect of\\nautonomy have been proposed, such as robust controllers or control barrier\\nfunctions. However, these solutions strongly rely on the model of the system\\nbeing available to the designer. As a parallel development, reinforcement\\nlearning provides model-agnostic control solutions but in general, it lacks the\\ntheoretical guarantees required for safety. Recent advances show that under\\nmild conditions, control policies can be learned via reinforcement learning,\\nwhich can be guaranteed to be safe by imposing these requirements as\\nconstraints of an optimization problem. However, to transfer from learning\\nsafety to learning safely, there are two hurdles that need to be overcome: (i)\\nit has to be possible to learn the policy without having to re-initialize the\\nsystem; and (ii) the rollouts of the system need to be in themselves safe. In\\nthis paper, we tackle the first issue, proposing an algorithm capable of\\noperating in the continuing task setting without the need of restarts. We\\nevaluate our approach in a numerical example, which shows the capabilities of\\nthe proposed approach in learning safe policies via safe exploration.',\n",
       "        b'Recent advancements in Artificial intelligence, especially deep learning, has\\nchanged many fields irreversibly by introducing state of the art methods for\\nautomation. Construction monitoring has not been an exception; as a part of\\nconstruction monitoring systems, material classification and recognition have\\ndrawn the attention of deep learning and machine vision researchers. However,\\nto create production-ready systems, there is still a long path to cover.\\nReal-world problems such as varying illuminations and reaching acceptable\\naccuracies need to be addressed in order to create robust systems. In this\\npaper, we have addressed these issues and reached a state of the art\\nperformance, i.e., 97.35% accuracy rate for this task. Also, a new dataset\\ncontaining 1231 images of 11 classes taken from several construction sites is\\ngathered and publicly published to help other researchers in this field.',\n",
       "        b'Time-series generated by end-users, edge devices, and different wearables are\\nmostly unlabelled. We propose a method to auto-generate labels of un-labelled\\ntime-series, exploiting very few representative labelled time-series. Our\\nmethod is based on representation learning using Auto Encoded Compact Sequence\\n(AECS) with a choice of best distance measure. It performs self-correction in\\niterations, by learning latent structure, as well as synthetically boosting\\nrepresentative time-series using Variational-Auto-Encoder (VAE) to improve the\\nquality of labels. We have experimented with UCR and UCI archives, public\\nreal-world univariate, multivariate time-series taken from different\\napplication domains. Experimental results demonstrate that the proposed method\\nis very close to the performance achieved by fully supervised classification.\\nThe proposed method not only produces close to benchmark results but\\noutperforms the benchmark performance in some cases.',\n",
       "        b'Schizophrenia is a complex psychiatric disorder involving changes in thought\\npatterns, perception, mood, and behavior. The diagnosis of schizophrenia is\\nchallenging and requires that patients show two or more positive symptoms for\\nat least one month. Delays in identifying this debilitating disorder can impede\\na patient ability to receive much needed treatment. Advances in neuroimaging\\nand machine learning algorithms can facilitate the diagnosis of schizophrenia\\nand help clinicians to provide an accurate diagnosis of the disease. This paper\\npresents a methodology for analyzing spectral images of Electroencephalography\\ncollected from patients with schizophrenia using convolutional neural networks.\\nIt also explains how we have developed accurate classifiers employing\\nModel-Agnostic Meta-Learning and prototypical networks. Such classifiers have\\nthe capacity to distinguish people with schizophrenia from healthy controls\\nbased on their brain activity.',\n",
       "        b'Unsupervised learning for geometric perception (depth, optical flow, etc.) is\\nof great interest to autonomous systems. Recent works on unsupervised learning\\nhave made considerable progress on perceiving geometry; however, they usually\\nignore the coherence of objects and perform poorly under scenarios with dark\\nand noisy environments. In contrast, supervised learning algorithms, which are\\nrobust, require large labeled geometric dataset. This paper introduces SIGNet,\\na novel framework that provides robust geometry perception without requiring\\ngeometrically informative labels. Specifically, SIGNet integrates semantic\\ninformation to make depth and flow predictions consistent with objects and\\nrobust to low lighting conditions. SIGNet is shown to improve upon the\\nstate-of-the-art unsupervised learning for depth prediction by 30% (in squared\\nrelative error). In particular, SIGNet improves the dynamic object class\\nperformance by 39% in depth prediction and 29% in flow prediction. Our code\\nwill be made available at https://github.com/mengyuest/SIGNet',\n",
       "        b'Vision-based methods for visibility estimation can play a critical role in\\nreducing traffic accidents caused by fog and haze. To overcome the\\ndisadvantages of current visibility estimation methods, we present a novel\\ndata-driven approach based on Gaussian image entropy and piecewise stationary\\ntime series analysis (SPEV). This is the first time that Gaussian image entropy\\nis used for estimating atmospheric visibility. To lessen the impact of\\nlandscape and sunshine illuminance on visibility estimation, we used region of\\ninterest (ROI) analysis and took into account relative ratios of image entropy,\\nto improve estimation accuracy. We assume fog and haze cause blurred images and\\nthat fog and haze can be considered as a piecewise stationary signal. We used\\npiecewise stationary time series analysis to construct the piecewise causal\\nrelationship between image entropy and visibility. To obtain a real-world\\nvisibility measure during fog and haze, a subjective assessment was established\\nthrough a study with 36 subjects who performed visibility observations.\\nFinally, a total of two million videos were used for training the SPEV model\\nand validate its effectiveness. The videos were collected from the constantly\\nfoggy and hazy Tongqi expressway in Jiangsu, China. The contrast model of\\nvisibility estimation was used for algorithm performance comparison, and the\\nvalidation results of the SPEV model were encouraging as 99.14% of the relative\\nerrors were less than 10%.',\n",
       "        b'Graph neural networks have shown significant success in the field of graph\\nrepresentation learning. Graph convolutions perform neighborhood aggregation\\nand represent one of the most important graph operations. Nevertheless, one\\nlayer of these neighborhood aggregation methods only consider immediate\\nneighbors, and the performance decreases when going deeper to enable larger\\nreceptive fields. Several recent studies attribute this performance\\ndeterioration to the over-smoothing issue, which states that repeated\\npropagation makes node representations of different classes indistinguishable.\\nIn this work, we study this observation systematically and develop new insights\\ntowards deeper graph neural networks. First, we provide a systematical analysis\\non this issue and argue that the key factor compromising the performance\\nsignificantly is the entanglement of representation transformation and\\npropagation in current graph convolution operations. After decoupling these two\\noperations, deeper graph neural networks can be used to learn graph node\\nrepresentations from larger receptive fields. We further provide a theoretical\\nanalysis of the above observation when building very deep models, which can\\nserve as a rigorous and gentle description of the over-smoothing issue. Based\\non our theoretical and empirical analysis, we propose Deep Adaptive Graph\\nNeural Network (DAGNN) to adaptively incorporate information from large\\nreceptive fields. A set of experiments on citation, co-authorship, and\\nco-purchase datasets have confirmed our analysis and insights and demonstrated\\nthe superiority of our proposed methods.',\n",
       "        b'Deep learning based single image super-resolution (SR) methods have been\\nrapidly evolved over the past few years and have yielded state-of-the-art\\nperformances over conventional methods. Since these methods usually minimized\\nl1 loss between the output SR image and the ground truth image, they yielded\\nvery high peak signal-to-noise ratio (PSNR) that is inversely proportional to\\nthese losses. Unfortunately, minimizing these losses inevitably lead to blurred\\nedges due to averaging of plausible solutions. Recently, SRGAN was proposed to\\navoid this average effect by minimizing perceptual losses instead of l1 loss\\nand it yielded perceptually better SR images (or images with sharp edges) at\\nthe price of lowering PSNR. In this paper, we propose SREdgeNet, edge enhanced\\nsingle image SR network, that was inspired by conventional SR theories so that\\naverage effect could be avoided not by changing the loss, but by changing the\\nSR network property with the same l1 loss. Our SREdgeNet consists of 3\\nsequential deep neural network modules: the first module is any\\nstate-of-the-art SR network and we selected a variant of EDSR. The second\\nmodule is any edge detection network taking the output of the first SR module\\nas an input and we propose DenseEdgeNet for this module. Lastly, the third\\nmodule is merging the outputs of the first and second modules to yield edge\\nenhanced SR image and we propose MergeNet for this module. Qualitatively, our\\nproposed method yielded images with sharp edges compared to other\\nstate-of-the-art SR methods. Quantitatively, our SREdgeNet yielded\\nstate-of-the-art performance in terms of structural similarity (SSIM) while\\nmaintained comparable PSNR for x8 enlargement.',\n",
       "        b'Analyzing the interactions between humans and objects from a video includes\\nidentification of the relationships between humans and the objects present in\\nthe video. It can be thought of as a specialized version of Visual Relationship\\nDetection, wherein one of the objects must be a human. While traditional\\nmethods formulate the problem as inference on a sequence of video segments, we\\npresent a hierarchical approach, LIGHTEN, to learn visual features to\\neffectively capture spatio-temporal cues at multiple granularities in a video.\\nUnlike current approaches, LIGHTEN avoids using ground truth data like depth\\nmaps or 3D human pose, thus increasing generalization across non-RGBD datasets\\nas well. Furthermore, we achieve the same using only the visual features,\\ninstead of the commonly used hand-crafted spatial features. We achieve\\nstate-of-the-art results in human-object interaction detection (88.9% and\\n92.6%) and anticipation tasks of CAD-120 and competitive results on image based\\nHOI detection in V-COCO dataset, setting a new benchmark for visual features\\nbased approaches. Code for LIGHTEN is available at\\nhttps://github.com/praneeth11009/LIGHTEN-Learning-Interactions-with-Graphs-and-Hierarchical-TEmporal-Networks-for-HOI',\n",
       "        b'We present a deep learning pipeline that leverages network self-prior to\\nrecover a full 3D model consisting of both a triangular mesh and a texture map\\nfrom the colored 3D point cloud. Different from previous methods either\\nexploiting 2D self-prior for image editing or 3D self-prior for pure surface\\nreconstruction, we propose to exploit a novel hybrid 2D-3D self-prior in deep\\nneural networks to significantly improve the geometry quality and produce a\\nhigh-resolution texture map, which is typically missing from the output of\\ncommodity-level 3D scanners. In particular, we first generate an initial mesh\\nusing a 3D convolutional neural network with 3D self-prior, and then encode\\nboth 3D information and color information in the 2D UV atlas, which is further\\nrefined by 2D convolutional neural networks with the self-prior. In this way,\\nboth 2D and 3D self-priors are utilized for the mesh and texture recovery.\\nExperiments show that, without the need of any additional training data, our\\nmethod recovers the 3D textured mesh model of high quality from sparse input,\\nand outperforms the state-of-the-art methods in terms of both the geometry and\\ntexture quality.',\n",
       "        b'Facial expression synthesis has achieved remarkable advances with the advent\\nof Generative Adversarial Networks (GANs). However, GAN-based approaches mostly\\ngenerate photo-realistic results as long as the testing data distribution is\\nclose to the training data distribution. The quality of GAN results\\nsignificantly degrades when testing images are from a slightly different\\ndistribution. Moreover, recent work has shown that facial expressions can be\\nsynthesized by changing localized face regions. In this work, we propose a\\npixel-based facial expression synthesis method in which each output pixel\\nobserves only one input pixel. The proposed method achieves good generalization\\ncapability by leveraging only a few hundred training images. Experimental\\nresults demonstrate that the proposed method performs comparably well against\\nstate-of-the-art GANs on in-dataset images and significantly better on\\nout-of-dataset images. In addition, the proposed model is two orders of\\nmagnitude smaller which makes it suitable for deployment on\\nresource-constrained devices.',\n",
       "        b'We introduce the framework of continuous--depth graph neural networks (GNNs).\\nGraph neural ordinary differential equations (GDEs) are formalized as the\\ncounterpart to GNNs where the input-output relationship is determined by a\\ncontinuum of GNN layers, blending discrete topological structures and\\ndifferential equations. The proposed framework is shown to be compatible with\\nvarious static and autoregressive GNN models. Results prove general\\neffectiveness of GDEs: in static settings they offer computational advantages\\nby incorporating numerical methods in their forward pass; in dynamic settings,\\non the other hand, they are shown to improve performance by exploiting the\\ngeometry of the underlying dynamics.',\n",
       "        b'The goal of this paper is to formulate a general framework for a\\nconstraint-based refinement of the optical flow using variational methods. We\\ndemonstrate that for a particular choice of the constraint, formulated as a\\nminimization problem with the quadratic regularization, our results are close\\nto the continuity equation based fluid flow. This closeness to the continuity\\nmodel is theoretically justified through a modified augmented Lagrangian method\\nand validated numerically. Further, along with the continuity constraint, our\\nmodel can include geometric constraints as well. The correctness of our process\\nis studied in the Hilbert space setting. Moreover, a special feature of our\\nsystem is the possibility of a diagonalization by the Cauchy-Riemann operator\\nand transforming it to a diffusion process on the curl and the divergence of\\nthe flow. Using the theory of semigroups on the decoupled system, we show that\\nour process preserves the spatial characteristics of the divergence and the\\nvorticities. We perform several numerical experiments and show the results on\\ndifferent datasets.',\n",
       "        b'Graph convolutional networks (GCN) have recently demonstrated their potential\\nin analyzing non-grid structure data that can be represented as graphs. The\\ncore idea is to encode the local topology of a graph, via convolutions, into\\nthe feature of a center node. In this paper, we propose a novel GCN model,\\nwhich we term as Shortest Path Graph Attention Network (SPAGAN). Unlike\\nconventional GCN models that carry out node-based attentions within each layer,\\nthe proposed SPAGAN conducts path-based attention that explicitly accounts for\\nthe influence of a sequence of nodes yielding the minimum cost, or shortest\\npath, between the center node and its higher-order neighbors. SPAGAN therefore\\nallows for a more informative and intact exploration of the graph structure and\\nfurther {a} more effective aggregation of information from distant neighbors\\ninto the center node, as compared to node-based GCN methods. We test SPAGAN on\\nthe downstream classification task on several standard datasets, and achieve\\nperformances superior to the state of the art. Code is publicly available at\\nhttps://github.com/ihollywhy/SPAGAN.',\n",
       "        b'We propose a generalization of transformer neural network architecture for\\narbitrary graphs. The original transformer was designed for Natural Language\\nProcessing (NLP), which operates on fully connected graphs representing all\\nconnections between the words in a sequence. Such architecture does not\\nleverage the graph connectivity inductive bias, and can perform poorly when the\\ngraph topology is important and has not been encoded into the node features. We\\nintroduce a graph transformer with four new properties compared to the standard\\nmodel. First, the attention mechanism is a function of the neighborhood\\nconnectivity for each node in the graph. Second, the positional encoding is\\nrepresented by the Laplacian eigenvectors, which naturally generalize the\\nsinusoidal positional encodings often used in NLP. Third, the layer\\nnormalization is replaced by a batch normalization layer, which provides faster\\ntraining and better generalization performance. Finally, the architecture is\\nextended to edge feature representation, which can be critical to tasks s.a.\\nchemistry (bond type) or link prediction (entity relationship in knowledge\\ngraphs). Numerical experiments on a graph benchmark demonstrate the performance\\nof the proposed graph transformer architecture. This work closes the gap\\nbetween the original transformer, which was designed for the limited case of\\nline graphs, and graph neural networks, that can work with arbitrary graphs. As\\nour architecture is simple and generic, we believe it can be used as a black\\nbox for future applications that wish to consider transformer and graphs.',\n",
       "        b'Recent works in speech recognition rely either on connectionist temporal\\nclassification (CTC) or sequence-to-sequence models for character-level\\nrecognition. CTC assumes conditional independence of individual characters,\\nwhereas attention-based models can provide nonsequential alignments. Therefore,\\nwe could use a CTC loss in combination with an attention-based model in order\\nto force monotonic alignments and at the same time get rid of the conditional\\nindependence assumption. In this paper, we use the recently proposed hybrid\\nCTC/attention architecture for audio-visual recognition of speech in-the-wild.\\nTo the best of our knowledge, this is the first time that such a hybrid\\narchitecture architecture is used for audio-visual recognition of speech. We\\nuse the LRS2 database and show that the proposed audio-visual model leads to an\\n1.3% absolute decrease in word error rate over the audio-only model and\\nachieves the new state-of-the-art performance on LRS2 database (7% word error\\nrate). We also observe that the audio-visual model significantly outperforms\\nthe audio-based model (up to 32.9% absolute improvement in word error rate) for\\nseveral different types of noise as the signal-to-noise ratio decreases.',\n",
       "        b'The recently-introduced class of ordinary differential equation networks\\n(ODE-Nets) establishes a fruitful connection between deep learning and\\ndynamical systems. In this work, we reconsider formulations of the weights as\\ncontinuous-depth functions using linear combinations of basis functions. This\\nperspective allows us to compress the weights through a change of basis,\\nwithout retraining, while maintaining near state-of-the-art performance. In\\nturn, both inference time and the memory footprint are reduced, enabling quick\\nand rigorous adaptation between computational environments. Furthermore, our\\nframework enables meaningful continuous-in-time batch normalization layers\\nusing function projections. The performance of basis function compression is\\ndemonstrated by applying continuous-depth models to (a) image classification\\ntasks using convolutional units and (b) sentence-tagging tasks using\\ntransformer encoder units.',\n",
       "        b'Capsule networks are a recently developed class of neural networks that\\npotentially address some of the deficiencies with traditional convolutional\\nneural networks. By replacing the standard scalar activations with vectors, and\\nby connecting the artificial neurons in a new way, capsule networks aim to be\\nthe next great development for computer vision applications. However, in order\\nto determine whether these networks truly operate differently than traditional\\nnetworks, one must look at the differences in the capsule features. To this\\nend, we perform several analyses with the purpose of elucidating capsule\\nfeatures and determining whether they perform as described in the initial\\npublication. First, we perform a deep visualization analysis to visually\\ncompare capsule features and convolutional neural network features. Then, we\\nlook at the ability for capsule features to encode information across the\\nvector components and address what changes in the capsule architecture provides\\nthe most benefit. Finally, we look at how well the capsule features are able to\\nencode instantiation parameters of class objects via visual transformations.',\n",
       "        b'In complex visual recognition tasks it is typical to adopt multiple\\ndescriptors, that describe different aspects of the images, for obtaining an\\nimproved recognition performance. Descriptors that have diverse forms can be\\nfused into a unified feature space in a principled manner using kernel methods.\\nSparse models that generalize well to the test data can be learned in the\\nunified kernel space, and appropriate constraints can be incorporated for\\napplication in supervised and unsupervised learning. In this paper, we propose\\nto perform sparse coding and dictionary learning in the multiple kernel space,\\nwhere the weights of the ensemble kernel are tuned based on graph-embedding\\nprinciples such that class discrimination is maximized. In our proposed\\nalgorithm, dictionaries are inferred using multiple levels of 1-D subspace\\nclustering in the kernel space, and the sparse codes are obtained using a\\nsimple levelwise pursuit scheme. Empirical results for object recognition and\\nimage clustering show that our algorithm outperforms existing sparse coding\\nbased approaches, and compares favorably to other state-of-the-art methods.',\n",
       "        b'Anomaly and outlier detection in datasets is a long-standing problem in\\nmachine learning. In some cases, anomaly detection is easy, such as when data\\nare drawn from well-characterized distributions such as the Gaussian. However,\\nwhen data occupy high-dimensional spaces, anomaly detection becomes more\\ndifficult. We present CLAM (Clustered Learning of Approximate Manifolds), a\\nfast hierarchical clustering technique that learns a manifold in a Banach space\\ndefined by a distance metric. CLAM induces a graph from the cluster tree, based\\non overlapping clusters determined by several geometric and topological\\nfeatures. On these graphs, we implement CHAODA (Clustered Hierarchical Anomaly\\nand Outlier Detection Algorithms), exploring various properties of the graphs\\nand their constituent clusters to compute scores of anomalousness. On 24\\npublicly available datasets, we compare the performance of CHAODA (by measure\\nof ROC AUC) to a variety of state-of-the-art unsupervised anomaly-detection\\nalgorithms. Six of the datasets are used for training. CHAODA outperforms other\\napproaches on 14 of the remaining 18 datasets.',\n",
       "        b'Attribution editing has achieved remarkable progress in recent years owing to\\nthe encoder-decoder structure and generative adversarial network (GAN).\\nHowever, it remains challenging in generating high-quality images with accurate\\nattribute transformation. Attacking these problems, the work proposes a novel\\nselective attribute editing model based on classification adversarial network\\n(referred to as ClsGAN) that shows good balance between attribute transfer\\naccuracy and photo-realistic images. Considering that the editing images are\\nprone to be affected by original attribute due to skip-connection in\\nencoder-decoder structure, an upper convolution residual network (referred to\\nas Tr-resnet) is presented to selectively extract information from the source\\nimage and target label. In addition, to further improve the transfer accuracy\\nof generated images, an attribute adversarial classifier (referred to as\\nAtta-cls) is introduced to guide the generator from the perspective of\\nattribute through learning the defects of attribute transfer images.\\nExperimental results on CelebA demonstrate that our ClsGAN performs favorably\\nagainst state-of-the-art approaches in image quality and transfer accuracy.\\nMoreover, ablation studies are also designed to verify the great performance of\\nTr-resnet and Atta-cls.',\n",
       "        b\"Deep neural networks (DNNs) are known vulnerable to backdoor attacks, a\\ntraining time attack that injects a trigger pattern into a small proportion of\\ntraining data so as to control the model's prediction at the test time.\\nBackdoor attacks are notably dangerous since they do not affect the model's\\nperformance on clean examples, yet can fool the model to make incorrect\\nprediction whenever the trigger pattern appears during testing. In this paper,\\nwe propose a novel defense framework Neural Attention Distillation (NAD) to\\nerase backdoor triggers from backdoored DNNs. NAD utilizes a teacher network to\\nguide the finetuning of the backdoored student network on a small clean subset\\nof data such that the intermediate-layer attention of the student network\\naligns with that of the teacher network. The teacher network can be obtained by\\nan independent finetuning process on the same clean subset. We empirically\\nshow, against 6 state-of-the-art backdoor attacks, NAD can effectively erase\\nthe backdoor triggers using only 5\\\\% clean training data without causing\\nobvious performance degradation on clean examples. Code is available in\\nhttps://github.com/bboylyg/NAD.\",\n",
       "        b\"We prove that the ordinary least-squares (OLS) estimator attains nearly\\nminimax optimal performance for the identification of linear dynamical systems\\nfrom a single observed trajectory. Our upper bound relies on a generalization\\nof Mendelson's small-ball method to dependent data, eschewing the use of\\nstandard mixing-time arguments. Our lower bounds reveal that these upper bounds\\nmatch up to logarithmic factors. In particular, we capture the correct\\nsignal-to-noise behavior of the problem, showing that more unstable linear\\nsystems are easier to estimate. This behavior is qualitatively different from\\narguments which rely on mixing-time calculations that suggest that unstable\\nsystems are more difficult to estimate. We generalize our technique to provide\\nbounds for a more general class of linear response time-series.\",\n",
       "        b'Last-generation GAN models allow to generate synthetic images which are\\nvisually indistinguishable from natural ones, raising the need to develop tools\\nto distinguish fake and natural images thus contributing to preserve the\\ntrustworthiness of digital images. While modern GAN models can generate very\\nhigh-quality images with no visible spatial artifacts, reconstruction of\\nconsistent relationships among colour channels is expectedly more difficult. In\\nthis paper, we propose a method for distinguishing GAN-generated from natural\\nimages by exploiting inconsistencies among spectral bands, with specific focus\\non the generation of synthetic face images. Specifically, we use cross-band\\nco-occurrence matrices, in addition to spatial co-occurrence matrices, as input\\nto a CNN model, which is trained to distinguish between real and synthetic\\nfaces. The results of our experiments confirm the goodness of our approach\\nwhich outperforms a similar detection technique based on intra-band spatial\\nco-occurrences only. The performance gain is particularly significant with\\nregard to robustness against post-processing, like geometric transformations,\\nfiltering and contrast manipulations.',\n",
       "        b'We propose SelfDoc, a task-agnostic pre-training framework for document image\\nunderstanding. Because documents are multimodal and are intended for sequential\\nreading, our framework exploits the positional, textual, and visual information\\nof every semantically meaningful component in a document, and it models the\\ncontextualization between each block of content. Unlike existing document\\npre-training models, our model is coarse-grained instead of treating individual\\nwords as input, therefore avoiding an overly fine-grained with excessive\\ncontextualization. Beyond that, we introduce cross-modal learning in the model\\npre-training phase to fully leverage multimodal information from unlabeled\\ndocuments. For downstream usage, we propose a novel modality-adaptive attention\\nmechanism for multimodal feature fusion by adaptively emphasizing language and\\nvision signals. Our framework benefits from self-supervised pre-training on\\ndocuments without requiring annotations by a feature masking training strategy.\\nIt achieves superior performance on multiple downstream tasks with\\nsignificantly fewer document images used in the pre-training stage compared to\\nprevious works.',\n",
       "        b'We propose a theoretical framework for approximate planning and learning in\\npartially observed systems. Our framework is based on the fundamental notion of\\ninformation state. We provide two equivalent definitions of information state\\n-- i) a function of history which is sufficient to compute the expected reward\\nand predict its next value; ii) equivalently, a function of the history which\\ncan be recursively updated and is sufficient to compute the expected reward and\\npredict the next observation. An information state always leads to a dynamic\\nprogramming decomposition. Our key result is to show that if a function of the\\nhistory (called approximate information state (AIS)) approximately satisfies\\nthe properties of the information state, then there is a corresponding\\napproximate dynamic program. We show that the policy computed using this is\\napproximately optimal with bounded loss of optimality. We show that several\\napproximations in state, observation and action spaces in literature can be\\nviewed as instances of AIS. In some of these cases, we obtain tighter bounds. A\\nsalient feature of AIS is that it can be learnt from data. We present AIS based\\nmulti-time scale policy gradient algorithms. and detailed numerical experiments\\nwith low, moderate and high dimensional environments.',\n",
       "        b'Cyber-physical systems of today are generating large volumes of time-series\\ndata. As manual inspection of such data is not tractable, the need for learning\\nmethods to help discover logical structure in the data has increased. We\\npropose a logic-based framework that allows domain-specific knowledge to be\\nembedded into formulas in a parametric logical specification over time-series\\ndata. The key idea is to then map a time series to a surface in the parameter\\nspace of the formula. Given this mapping, we identify the Hausdorff distance\\nbetween boundaries as a natural distance metric between two time-series data\\nunder the lens of the parametric specification. This enables embedding\\nnon-trivial domain-specific knowledge into the distance metric and then using\\noff-the-shelf machine learning tools to label the data. After labeling the\\ndata, we demonstrate how to extract a logical specification for each label.\\nFinally, we showcase our technique on real world traffic data to learn\\nclassifiers/monitors for slow-downs and traffic jams.',\n",
       "        b'The segmentation of retinal vessels is of significance for doctors to\\ndiagnose the fundus diseases. However, existing methods have various problems\\nin the segmentation of the retinal vessels, such as insufficient segmentation\\nof retinal vessels, weak anti-noise interference ability, and sensitivity to\\nlesions, etc. Aiming to the shortcomings of existed methods, this paper\\nproposes the use of conditional deep convolutional generative adversarial\\nnetworks to segment the retinal vessels. We mainly improve the network\\nstructure of the generator. The introduction of the residual module at the\\nconvolutional layer for residual learning makes the network structure sensitive\\nto changes in the output, as to better adjust the weight of the generator. In\\norder to reduce the number of parameters and calculations, using a small\\nconvolution to halve the number of channels in the input signature before using\\na large convolution kernel. By used skip connection to connect the output of\\nthe convolutional layer with the output of the deconvolution layer to avoid\\nlow-level information sharing. By verifying the method on the DRIVE and STARE\\ndatasets, the segmentation accuracy rate is 96.08% and 97.71%, the sensitivity\\nreaches 82.74% and 85.34% respectively, and the F-measure reaches 82.08% and\\n85.02% respectively. The sensitivity is 4.82% and 2.4% higher than that of\\nR2U-Net.',\n",
       "        b'While Generative Adversarial Networks (GANs) have demonstrated promising\\nperformance on multiple vision tasks, their learning dynamics are not yet well\\nunderstood, both in theory and in practice. To address this issue, we study GAN\\ndynamics in a simple yet rich parametric model that exhibits several of the\\ncommon problematic convergence behaviors such as vanishing gradients, mode\\ncollapse, and diverging or oscillatory behavior. In spite of the non-convex\\nnature of our model, we are able to perform a rigorous theoretical analysis of\\nits convergence behavior. Our analysis reveals an interesting dichotomy: a GAN\\nwith an optimal discriminator provably converges, while first order\\napproximations of the discriminator steps lead to unstable GAN dynamics and\\nmode collapse. Our result suggests that using first order discriminator steps\\n(the de-facto standard in most existing GAN setups) might be one of the factors\\nthat makes GAN training challenging in practice.',\n",
       "        b'Modern computer vision algorithms typically require expensive data\\nacquisition and accurate manual labeling. In this work, we instead leverage the\\nrecent progress in computer graphics to generate fully labeled, dynamic, and\\nphoto-realistic proxy virtual worlds. We propose an efficient real-to-virtual\\nworld cloning method, and validate our approach by building and publicly\\nreleasing a new video dataset, called Virtual KITTI (see\\nhttp://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds),\\nautomatically labeled with accurate ground truth for object detection,\\ntracking, scene and instance segmentation, depth, and optical flow. We provide\\nquantitative experimental evidence suggesting that (i) modern deep learning\\nalgorithms pre-trained on real data behave similarly in real and virtual\\nworlds, and (ii) pre-training on virtual data improves performance. As the gap\\nbetween real and virtual worlds is small, virtual worlds enable measuring the\\nimpact of various weather and imaging conditions on recognition performance,\\nall other things being equal. We show these factors may affect drastically\\notherwise high-performing deep models for tracking.',\n",
       "        b'We address the problem of unsupervised learning of complex articulated object\\nmodels from 3D range data. We describe an algorithm whose input is a set of\\nmeshes corresponding to different configurations of an articulated object. The\\nalgorithm automatically recovers a decomposition of the object into\\napproximately rigid parts, the location of the parts in the different object\\ninstances, and the articulated object skeleton linking the parts. Our algorithm\\nfirst registers allthe meshes using an unsupervised non-rigid technique\\ndescribed in a companion paper. It then segments the meshes using a graphical\\nmodel that captures the spatial contiguity of parts. The segmentation is done\\nusing the EM algorithm, iterating between finding a decomposition of the object\\ninto rigid parts, and finding the location of the parts in the object\\ninstances. Although the graphical model is densely connected, the object\\ndecomposition step can be performed optimally and efficiently, allowing us to\\nidentify a large number of object parts while avoiding local maxima. We\\ndemonstrate the algorithm on real world datasets, recovering a 15-part\\narticulated model of a human puppet from just 7 different puppet\\nconfigurations, as well as a 4 part model of a fiexing arm where significant\\nnon-rigid deformation was present.',\n",
       "        b'Generative adversarial networks (GANs) have demonstrated great success in\\ngenerating various visual content. However, images generated by existing GANs\\nare often of attributes (e.g., smiling expression) learned from one image\\ndomain. As a result, generating images of multiple attributes requires many\\nreal samples possessing multiple attributes which are very resource expensive\\nto be collected. In this paper, we propose a novel GAN, namely IntersectGAN, to\\nlearn multiple attributes from different image domains through an intersecting\\narchitecture. For example, given two image domains $X_1$ and $X_2$ with certain\\nattributes, the intersection $X_1 \\\\cap X_2$ denotes a new domain where images\\npossess the attributes from both $X_1$ and $X_2$ domains. The proposed\\nIntersectGAN consists of two discriminators $D_1$ and $D_2$ to distinguish\\nbetween generated and real samples of different domains, and three generators\\nwhere the intersection generator is trained against both discriminators. And an\\noverall adversarial loss function is defined over three generators. As a\\nresult, our proposed IntersectGAN can be trained on multiple domains of which\\neach presents one specific attribute, and eventually eliminates the need of\\nreal sample images simultaneously possessing multiple attributes. By using the\\nCelebFaces Attributes dataset, our proposed IntersectGAN is able to produce\\nhigh quality face images possessing multiple attributes (e.g., a face with\\nblack hair and a smiling expression). Both qualitative and quantitative\\nevaluations are conducted to compare our proposed IntersectGAN with other\\nbaseline methods. Besides, several different applications of IntersectGAN have\\nbeen explored with promising results.',\n",
       "        b'Localizing functional regions of objects or affordances is an important\\naspect of scene understanding. In this work, we cast the problem of affordance\\nsegmentation as that of semantic image segmentation. In order to explore\\nvarious levels of supervision, we introduce a pixel-annotated affordance\\ndataset of 3090 images containing 9916 object instances with rich contextual\\ninformation in terms of human-object interactions. We use a deep convolutional\\nneural network within an expectation maximization framework to take advantage\\nof weakly labeled data like image level annotations or keypoint annotations. We\\nshow that a further reduction in supervision is possible with a minimal loss in\\nperformance when human pose is used as context.',\n",
       "        b'Delineation of line patterns in images is a basic step required in various\\napplications such as blood vessel detection in medical images, segmentation of\\nrivers or roads in aerial images, detection of cracks in walls or pavements,\\netc. In this paper we present trainable B-COSFIRE filters, which are a model of\\nsome neurons in area V1 of the primary visual cortex, and apply it to the\\ndelineation of line patterns in different kinds of images. B-COSFIRE filters\\nare trainable as their selectivity is determined in an automatic configuration\\nprocess given a prototype pattern of interest. They are configurable to detect\\nany preferred line structure (e.g. segments, corners, cross-overs, etc.), so\\nusable for automatic data representation learning. We carried out experiments\\non two data sets, namely a line-network data set from INRIA and a data set of\\nretinal fundus images named IOSTAR. The results that we achieved confirm the\\nrobustness of the proposed approach and its effectiveness in the delineation of\\nline structures in different kinds of images.',\n",
       "        b'Machine learning models in practical settings are typically confronted with\\nchanges to the distribution of the incoming data. Such changes can severely\\naffect the model performance, leading for example to misclassifications of\\ndata. This is particularly apparent in the domain of bionic hand prostheses,\\nwhere machine learning models promise faster and more intuitive user\\ninterfaces, but are hindered by their lack of robustness to everyday\\ndisturbances, such as electrode shifts. One way to address changes in the data\\ndistribution is transfer learning, that is, to transfer the disturbed data to a\\nspace where the original model is applicable again. In this contribution, we\\npropose a novel expectation maximization algorithm to learn linear\\ntransformations that maximize the likelihood of disturbed data after the\\ntransformation. We also show that this approach generalizes to discriminative\\nmodels, in particular learning vector quantization models. In our evaluation on\\ndata from the bionic prostheses domain we demonstrate that our approach can\\nlearn a transformation which improves classification accuracy significantly and\\noutperforms all tested baselines, if few data or few classes are available in\\nthe target domain.',\n",
       "        b'This paper introduces a method for image semantic segmentation grounded on a\\nnovel fusion scheme, which takes place inside a deep convolutional neural\\nnetwork. The main goal of our proposal is to explore object boundary\\ninformation to improve the overall segmentation performance. Unlike previous\\nworks that combine boundary and segmentation features, or those that use\\nboundary information to regularize semantic segmentation, we instead propose a\\nnovel approach that embodies boundary information onto segmentation. For that,\\nour semantic segmentation method uses two streams, which are combined through\\nan attention gate, forming an end-to-end Y-model. To the best of our knowledge,\\nours is the first work to show that boundary detection can improve semantic\\nsegmentation when fused through a semantic fusion gate (attention model). We\\nperformed an extensive evaluation of our method over public data sets. We found\\ncompetitive results on all data sets after comparing our proposed model with\\nother twelve state-of-the-art segmenters, considering the same training\\nconditions. Our proposed model achieved the best mIoU on the CityScapes,\\nCamVid, and Pascal Context data sets, and the second best on Mapillary Vistas.',\n",
       "        b'The objective of this paper is to recover the original component signals from\\na mixture audio with the aid of visual cues of the sound sources. Such task is\\nusually referred as visually guided sound source separation. The proposed\\nCascaded Opponent Filter (COF) framework consists of multiple stages, which\\nrecursively refine the source separation. A key element in COF is a novel\\nopponent filter module that identifies and relocates residual components\\nbetween sources. The system is guided by the appearance and motion of the\\nsource, and, for this purpose, we study different representations based on\\nvideo frames, optical flows, dynamic images, and their combinations. Finally,\\nwe propose a Sound Source Location Masking (SSLM) technique, which, together\\nwith COF, produces a pixel level mask of the source location. The entire system\\nis trained end-to-end using a large set of unlabelled videos. We compare COF\\nwith recent baselines and obtain the state-of-the-art performance in three\\nchallenging datasets (MUSIC, A-MUSIC, and A-NATURAL). Project page:\\nhttps://ly-zhu.github.io/cof-net.',\n",
       "        b'Time series classification is a task that aims at classifying chronological\\ndata. It is used in a diverse range of domains such as meteorology, medicine\\nand physics. In the last decade, many algorithms have been built to perform\\nthis task with very appreciable accuracy. However, applications where time\\nseries have uncertainty has been under-explored. Using uncertainty propagation\\ntechniques, we propose a new uncertain dissimilarity measure based on Euclidean\\ndistance. We then propose the uncertain shapelet transform algorithm for the\\nclassification of uncertain time series. The large experiments we conducted on\\nstate of the art datasets show the effectiveness of our contribution. The\\nsource code of our contribution and the datasets we used are all available on a\\npublic repository.',\n",
       "        b\"Zero-shot learning is a new paradigm to classify objects from classes that\\nare not available at training time. Zero-shot learning (ZSL) methods have\\nattracted considerable attention in recent years because of their ability to\\nclassify unseen/novel class examples. Most of the existing approaches on ZSL\\nworks when all the samples from seen classes are available to train the model,\\nwhich does not suit real life. In this paper, we tackle this hindrance by\\ndeveloping a generative replay-based continual ZSL (GRCZSL). The proposed\\nmethod endows traditional ZSL to learn from streaming data and acquire new\\nknowledge without forgetting the previous tasks' gained experience. We handle\\ncatastrophic forgetting in GRCZSL by replaying the synthetic samples of seen\\nclasses, which have appeared in the earlier tasks. These synthetic samples are\\nsynthesized using the trained conditional variational autoencoder (VAE) over\\nthe immediate past task. Moreover, we only require the current and immediate\\nprevious VAE at any time for training and testing. The proposed GRZSL method is\\ndeveloped for a single-head setting of continual learning, simulating a\\nreal-world problem setting. In this setting, task identity is given during\\ntraining but unavailable during testing. GRCZSL performance is evaluated on\\nfive benchmark datasets for the generalized setup of ZSL with fixed and dynamic\\n(incremental class) settings of continual learning. The existing class setting\\npresented recently in the literature is not suitable for a class-incremental\\nsetting. Therefore, this paper proposes a new setting to address this issue.\\nExperimental results show that the proposed method significantly outperforms\\nthe baseline and the state-of-the-art method and makes it more suitable for\\nreal-world applications.\",\n",
       "        b'Weakly-supervised object detection has recently attracted increasing\\nattention since it only requires image-levelannotations. However, the\\nperformance obtained by existingmethods is still far from being satisfactory\\ncompared with fully-supervised object detection methods. To achieve a good\\ntrade-off between annotation cost and object detection performance,we propose a\\nsimple yet effective method which incorporatesCNN visualization with click\\nsupervision to generate the pseudoground-truths (i.e., bounding boxes). These\\npseudo ground-truthscan be used to train a fully-supervised detector. To\\nestimatethe object scale, we firstly adopt a proposal selection algorithmto\\npreserve high-quality proposals, and then generate ClassActivation Maps (CAMs)\\nfor these preserved proposals by theproposed CNN visualization algorithm called\\nSpatial AttentionCAM. Finally, we fuse these CAMs together to generate\\npseudoground-truths and train a fully-supervised object detector withthese\\nground-truths. Experimental results on the PASCAL VOC2007 and VOC 2012 datasets\\nshow that the proposed methodcan obtain much higher accuracy for estimating the\\nobject scale,compared with the state-of-the-art image-level based methodsand\\nthe center-click based method',\n",
       "        b'We show how to construct variance-aware confidence sets for linear bandits\\nand linear mixture Markov Decision Process (MDP). Our method yields the\\nfollowing new regret bounds:\\n  * For linear bandits, we obtain an $\\\\widetilde{O}(\\\\mathrm{poly}(d)\\\\sqrt{1 +\\n\\\\sum_{i=1}^{K}\\\\sigma_i^2})$ regret bound, where $d$ is the feature dimension,\\n$K$ is the number of rounds, and $\\\\sigma_i^2$ is the (unknown) variance of the\\nreward at the $i$-th round. This is the first regret bound that only scales\\nwith the variance and the dimension, with no explicit polynomial dependency on\\n$K$.\\n  * For linear mixture MDP, we obtain an $\\\\widetilde{O}(\\\\mathrm{poly}(d, \\\\log\\nH)\\\\sqrt{K})$ regret bound, where $d$ is the number of base models, $K$ is the\\nnumber of episodes, and $H$ is the planning horizon. This is the first regret\\nbound that only scales logarithmically with $H$ in the reinforcement learning\\nwith linear function approximation setting, thus exponentially improving\\nexisting results.\\n  Our methods utilize three novel ideas that may be of independent interest: 1)\\napplications of the peeling techniques to the norm of input and the magnitude\\nof variance, 2) a recursion-based approach to estimate the variance, and 3) a\\nconvex potential lemma that somewhat generalizes the seminal elliptical\\npotential lemma.',\n",
       "        b'We describe a fully-automatic 3D-segmentation technique for brain MR images.\\nUsing Markov random fields the segmentation algorithm captures three important\\nMR features, i.e. non-parametric distributions of tissue intensities,\\nneighborhood correlations and signal inhomogeneities. Detailed simulations and\\nreal MR images demonstrate the performance of the segmentation algorithm. The\\nimpact of noise, inhomogeneity, smoothing and structure thickness is analyzed\\nquantitatively. Even single echo MR images are well classified into gray\\nmatter, white matter, cerebrospinal fluid, scalp-bone and background. A\\nsimulated annealing and an iterated conditional modes implementation are\\npresented.\\n  Keywords: Magnetic Resonance Imaging, Segmentation, Markov Random Fields',\n",
       "        b'Semantic segmentation is essentially important to biomedical image analysis.\\nMany recent works mainly focus on integrating the Fully Convolutional Network\\n(FCN) architecture with sophisticated convolution implementation and deep\\nsupervision. In this paper, we propose to decompose the single segmentation\\ntask into three subsequent sub-tasks, including (1) pixel-wise image\\nsegmentation, (2) prediction of the class labels of the objects within the\\nimage, and (3) classification of the scene the image belonging to. While these\\nthree sub-tasks are trained to optimize their individual loss functions of\\ndifferent perceptual levels, we propose to let them interact by the task-task\\ncontext ensemble. Moreover, we propose a novel sync-regularization to penalize\\nthe deviation between the outputs of the pixel-wise segmentation and the class\\nprediction tasks. These effective regularizations help FCN utilize context\\ninformation comprehensively and attain accurate semantic segmentation, even\\nthough the number of the images for training may be limited in many biomedical\\napplications. We have successfully applied our framework to three diverse 2D/3D\\nmedical image datasets, including Robotic Scene Segmentation Challenge 18\\n(ROBOT18), Brain Tumor Segmentation Challenge 18 (BRATS18), and Retinal Fundus\\nGlaucoma Challenge (REFUGE18). We have achieved top-tier performance in all\\nthree challenges.',\n",
       "        b'Despite the recent development in the topic of explainable AI/ML for image\\nand text data, the majority of current solutions are not suitable to explain\\nthe prediction of neural network models when the datasets are tabular and their\\nfeatures are in high-dimensional vectorized formats. To mitigate this\\nlimitation, therefore, we borrow two notable ideas (i.e., \"explanation by\\nintervention\" from causality and \"explanation are contrastive\" from philosophy)\\nand propose a novel solution, named as GRACE, that better explains neural\\nnetwork models\\' predictions for tabular datasets. In particular, given a\\nmodel\\'s prediction as label X, GRACE intervenes and generates a\\nminimally-modified contrastive sample to be classified as Y, with an intuitive\\ntextual explanation, answering the question of \"Why X rather than Y?\" We carry\\nout comprehensive experiments using eleven public datasets of different scales\\nand domains (e.g., # of features ranges from 5 to 216) and compare GRACE with\\ncompeting baselines on different measures: fidelity, conciseness, info-gain,\\nand influence. The user-studies show that our generated explanation is not only\\nmore intuitive and easy-to-understand but also facilitates end-users to make as\\nmuch as 60% more accurate post-explanation decisions than that of Lime.'],\n",
       "       dtype=object)>,\n",
       " <tf.Tensor: shape=(128, 497), dtype=int64, numpy=\n",
       " array([[0, 0, 1, ..., 0, 0, 0],\n",
       "        [0, 1, 1, ..., 0, 0, 0],\n",
       "        [0, 0, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 1, ..., 0, 0, 0]], dtype=int64)>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a75cd442-e9f9-44fe-ad33-467ed568e629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract: b'Computational results demonstrate that posterior sampling for reinforcement\\nlearning (PSRL) dramatically outperforms algorithms driven by optimism, such as\\nUCRL2. We provide insight into the extent of this performance boost and the\\nphenomenon that drives it. We leverage this insight to establish an\\n$\\\\tilde{O}(H\\\\sqrt{SAT})$ Bayesian expected regret bound for PSRL in\\nfinite-horizon episodic Markov decision processes, where $H$ is the horizon,\\n$S$ is the number of states, $A$ is the number of actions and $T$ is the time\\nelapsed. This improves upon the best previous bound of $\\\\tilde{O}(H S\\n\\\\sqrt{AT})$ for any reinforcement learning algorithm.'\n",
      "Label(s): ['cs.LG' 'stat.ML' 'cs.AI']\n",
      " \n",
      "Abstract: b'Facial Expression Recognition (FER) is a classification task that points to\\nface variants. Hence, there are certain affinity features between facial\\nexpressions, receiving little attention in the FER literature. Convolution\\npadding, despite helping capture the edge information, causes erosion of the\\nfeature map simultaneously. After multi-layer filling convolution, the output\\nfeature map named albino feature definitely weakens the representation of the\\nexpression. To tackle these challenges, we propose a novel architecture named\\nAmending Representation Module (ARM). ARM is a substitute for the pooling\\nlayer. Theoretically, it can be embedded in the back end of any network to deal\\nwith the Padding Erosion. ARM efficiently enhances facial expression\\nrepresentation from two different directions: 1) reducing the weight of eroded\\nfeatures to offset the side effect of padding, and 2) sharing affinity features\\nover mini-batch to strengthen the representation learning. Experiments on\\npublic benchmarks prove that our ARM boosts the performance of FER remarkably.\\nThe validation accuracies are respectively 92.05% on RAF-DB, 65.2% on\\nAffect-Net, and 58.71% on SFEW, exceeding current state-of-the-art methods. Our\\nimplementation and trained models are available at\\nhttps://github.com/JiaweiShiCV/Amend-Representation-Module.'\n",
      "Label(s): ['cs.CV']\n",
      " \n",
      "Abstract: b'We propose Attention Grounder (AttnGrounder), a single-stage end-to-end\\ntrainable model for the task of visual grounding. Visual grounding aims to\\nlocalize a specific object in an image based on a given natural language text\\nquery. Unlike previous methods that use the same text representation for every\\nimage region, we use a visual-text attention module that relates each word in\\nthe given query with every region in the corresponding image for constructing a\\nregion dependent text representation. Furthermore, for improving the\\nlocalization ability of our model, we use our visual-text attention module to\\ngenerate an attention mask around the referred object. The attention mask is\\ntrained as an auxiliary task using a rectangular mask generated with the\\nprovided ground-truth coordinates. We evaluate AttnGrounder on the Talk2Car\\ndataset and show an improvement of 3.26% over the existing methods.'\n",
      "Label(s): ['cs.CV']\n",
      " \n",
      "Abstract: b'Although hierarchical structures are popular in recent vision transformers,\\nthey require sophisticated designs and massive datasets to work well. In this\\nwork, we explore the idea of nesting basic local transformers on\\nnon-overlapping image blocks and aggregating them in a hierarchical manner. We\\nfind that the block aggregation function plays a critical role in enabling\\ncross-block non-local information communication. This observation leads us to\\ndesign a simplified architecture with minor code changes upon the original\\nvision transformer and obtains improved performance compared to existing\\nmethods. Our empirical results show that the proposed method NesT converges\\nfaster and requires much less training data to achieve good generalization. For\\nexample, a NesT with 68M parameters trained on ImageNet for 100/300 epochs\\nachieves $82.3\\\\%/83.8\\\\%$ accuracy evaluated on $224\\\\times 224$ image size,\\noutperforming previous methods with up to $57\\\\%$ parameter reduction. Training\\na NesT with 6M parameters from scratch on CIFAR10 achieves $96\\\\%$ accuracy\\nusing a single GPU, setting a new state of the art for vision transformers.\\nBeyond image classification, we extend the key idea to image generation and\\nshow NesT leads to a strong decoder that is 8$\\\\times$ faster than previous\\ntransformer based generators. Furthermore, we also propose a novel method for\\nvisually interpreting the learned model. Source code is available\\nhttps://github.com/google-research/nested-transformer.'\n",
      "Label(s): ['cs.CV']\n",
      " \n",
      "Abstract: b\"In collaborative intelligence applications, part of a deep neural network\\n(DNN) is deployed on a relatively low-complexity device such as a mobile phone\\nor edge device, and the remainder of the DNN is processed where more computing\\nresources are available, such as in the cloud. This paper presents a novel\\nlightweight compression technique designed specifically to code the activations\\nof a split DNN layer, while having a low complexity suitable for edge devices\\nand not requiring any retraining. We also present a modified\\nentropy-constrained quantizer design algorithm optimized for clipped\\nactivations. When applied to popular object-detection and classification DNNs,\\nwe were able to compress the 32-bit floating point activations down to 0.6 to\\n0.8 bits, while keeping the loss in accuracy to less than 1%. When compared to\\nHEVC, we found that the lightweight codec consistently provided better\\ninference accuracy, by up to 1.3%. The performance and simplicity of this\\nlightweight compression technique makes it an attractive option for coding a\\nlayer's activations in split neural networks for edge/cloud applications.\"\n",
      "Label(s): ['cs.LG' 'eess.IV']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def invert_multi_hot(encoded_labels):\n",
    "    hot_indices = np.argwhere(encoded_labels==1.0)[...,0]\n",
    "    return np.take(vocab, hot_indices)\n",
    "\n",
    "text_batch, label_batch = next(iter(train_dataset))\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Abstract: {text}\")\n",
    "    print(f\"Label(s): {invert_multi_hot(label[0])}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4428bad9-c022-46da-8815-53c829f46493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164146\n"
     ]
    }
   ],
   "source": [
    "# This code calculates the size of the vocabulary in the \"abstracts\" column of the train_df DataFrame.\n",
    "\n",
    "# Creating vocabulary with uniques words\n",
    "vocabulary = set()\n",
    "train_df[\"abstracts\"].str.lower().str.split().apply(vocabulary.update)\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8c3526-ba9d-4caf-a54a-26b08abb1d8d",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364cc0f-7227-437d-aa72-407d0f33273d",
   "metadata": {},
   "source": [
    "### In the Context of Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c54a3a41-4ad5-4fa2-914e-523d848fefc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = layers.TextVectorization(max_tokens=vocabulary_size,ngrams=2,output_mode=\"tf_idf\")\n",
    "text_vectorizer.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e680a9bc-e45c-493a-8f96-9556e0a465f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n",
    "validation_dataset = validation_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n",
    "test_dataset = test_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0cdbc8-6525-4276-a3ba-38c61e2d2c84",
   "metadata": {},
   "source": [
    "<h2>Model Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "629c54a9-a31f-4b87-9c61-c63a5051ba84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m949s\u001b[0m 2s/step - binary_accuracy: 0.9612 - loss: 0.0826 - val_binary_accuracy: 0.9983 - val_loss: 0.0059\n",
      "Epoch 2/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m914s\u001b[0m 2s/step - binary_accuracy: 0.9983 - loss: 0.0063 - val_binary_accuracy: 0.9985 - val_loss: 0.0049\n",
      "Epoch 3/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m892s\u001b[0m 2s/step - binary_accuracy: 0.9987 - loss: 0.0046 - val_binary_accuracy: 0.9987 - val_loss: 0.0046\n",
      "Epoch 4/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1044s\u001b[0m 3s/step - binary_accuracy: 0.9990 - loss: 0.0037 - val_binary_accuracy: 0.9987 - val_loss: 0.0046\n",
      "Epoch 5/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m899s\u001b[0m 2s/step - binary_accuracy: 0.9991 - loss: 0.0032 - val_binary_accuracy: 0.9988 - val_loss: 0.0047\n",
      "Epoch 6/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m893s\u001b[0m 2s/step - binary_accuracy: 0.9992 - loss: 0.0029 - val_binary_accuracy: 0.9988 - val_loss: 0.0047\n",
      "Epoch 7/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m913s\u001b[0m 2s/step - binary_accuracy: 0.9993 - loss: 0.0026 - val_binary_accuracy: 0.9988 - val_loss: 0.0047\n",
      "Epoch 8/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m896s\u001b[0m 2s/step - binary_accuracy: 0.9993 - loss: 0.0024 - val_binary_accuracy: 0.9988 - val_loss: 0.0048\n",
      "Epoch 9/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m888s\u001b[0m 2s/step - binary_accuracy: 0.9994 - loss: 0.0023 - val_binary_accuracy: 0.9989 - val_loss: 0.0048\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model1 = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dropout(0.5), \n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dropout(0.5), \n",
    "    layers.Dense(lookup.vocabulary_size(), activation='sigmoid')\n",
    "])\n",
    "\n",
    "model1.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['binary_accuracy'])\n",
    "es = EarlyStopping(patience=5,restore_best_weights=True)\n",
    "history = model1.fit(train_dataset,validation_data=validation_dataset,epochs=20,callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34ffd7f-3c10-4433-9b1a-578bed1c84ac",
   "metadata": {},
   "source": [
    "## Save Model and Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "24959197-fc76-4f09-8b22-2159c895ccee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "import pickle\n",
    "model1.save(\"models/model.h5\")\n",
    "\n",
    "# Save the configuration of the text vectorizer\n",
    "saved_text_vectorizer_config = text_vectorizer.get_config()\n",
    "with open(\"models/text_vectorizer_config.pkl\", \"wb\") as f:\n",
    "    pickle.dump(saved_text_vectorizer_config, f)\n",
    "\n",
    "\n",
    "# Save the vocabulary\n",
    "with open(\"models/vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ebe8a0-189f-47de-b78f-1ce9aa5fbfe6",
   "metadata": {},
   "source": [
    "### Load Model and Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "48babf27-ac95-4cf7-b86e-27edaffae660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempted to set a vocabulary larger than the maximum vocab size. Received vocabulary size is 164148; `max_tokens` is 164146.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 48\u001b[0m\n\u001b[0;32m     41\u001b[0m loaded_text_vectorizer \u001b[38;5;241m=\u001b[39m TextVectorization(\n\u001b[0;32m     42\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vocabulary),\n\u001b[0;32m     43\u001b[0m     output_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     44\u001b[0m     output_sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m  \u001b[38;5;66;03m# Adjust to your original setup\u001b[39;00m\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Set the vocabulary directly\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m loaded_text_vectorizer\u001b[38;5;241m.\u001b[39mset_vocabulary(vocabulary)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\text_vectorization.py:519\u001b[0m, in \u001b[0;36mTextVectorization.set_vocabulary\u001b[1;34m(self, vocabulary, idf_weights)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_vocabulary\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocabulary, idf_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    500\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sets vocabulary (and optionally document frequency) for this layer.\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \n\u001b[0;32m    502\u001b[0m \u001b[38;5;124;03m    This method sets the vocabulary and IDF weights for this layer directly,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03m            Should not be set otherwise.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lookup_layer\u001b[38;5;241m.\u001b[39mset_vocabulary(vocabulary, idf_weights\u001b[38;5;241m=\u001b[39midf_weights)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\index_lookup.py:478\u001b[0m, in \u001b[0;36mIndexLookup.set_vocabulary\u001b[1;34m(self, vocabulary, idf_weights)\u001b[0m\n\u001b[0;32m    476\u001b[0m new_vocab_size \u001b[38;5;241m=\u001b[39m token_start \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens)\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (new_vocab_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_tokens):\n\u001b[1;32m--> 478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to set a vocabulary larger than the maximum vocab \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize. Received vocabulary size is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_vocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`max_tokens` is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m     )\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lookup_table_from_tokens(tokens)\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_vocabulary_size()\n",
      "\u001b[1;31mValueError\u001b[0m: Attempted to set a vocabulary larger than the maximum vocab size. Received vocabulary size is 164148; `max_tokens` is 164146."
     ]
    }
   ],
   "source": [
    "# from tensorflow import keras\n",
    "# import pickle\n",
    "\n",
    "# # Load the model\n",
    "# loaded_model = keras.models.load_model(\"models/model.h5\")\n",
    "\n",
    "# from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# # Load the configuration of the text vectorizer\n",
    "# with open(\"models/text_vectorizer_config.pkl\", \"rb\") as f:\n",
    "#     saved_text_vectorizer_config = pickle.load(f)\n",
    "\n",
    "# # Create a new TextVectorization layer with the saved configuration\n",
    "# loaded_text_vectorizer = TextVectorization.from_config(saved_text_vectorizer_config)\n",
    "\n",
    "# # Load the saved weights into the new TextVectorization layer\n",
    "# with open(\"models/text_vectorizer_weights.pkl\", \"rb\") as f:\n",
    "#     weights = pickle.load(f)\n",
    "#     loaded_text_vectorizer.set_weights(weights)\n",
    "\n",
    "\n",
    "\n",
    "# Create a new TextVectorization layer with no token limit\n",
    "loaded_text_vectorizer = TextVectorization(\n",
    "    max_tokens=None,  # No limit on the vocabulary size\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=200  # Adjust to your original setup\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "2f4db8e2-16b6-4c5c-9062-5602c70bdd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabulary\n",
    "with open(\"models/vocab.pkl\", \"rb\") as f:\n",
    "    loaded_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0563b1a0-031a-40dd-a3ca-42ca2975e47b",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985259cb-dd95-40da-a6c1-10b5c14d2404",
   "metadata": {},
   "source": [
    "Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "d26316b6-bd7e-4696-b2e7-85b862760b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_multi_hot(encoded_labels):\n",
    "    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]\n",
    "    return np.take(loaded_vocab, hot_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "31aa108c-c7cc-4bd3-8c13-f3cd0936d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(abstract, model, vectorizer, label_lookup):\n",
    "    preprocessed_abstract = vectorizer([abstract])\n",
    "    predictions = model.predict(preprocessed_abstract)\n",
    "    predicted_labels = label_lookup(np.round(predictions).astype(int)[0])\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "99d6a7b5-78a7-4ca3-b7ca-ecfed980164c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Exception encountered when calling TextVectorization.call().\n\n\u001b[1m{{function_node __wrapped__LookupTableFindV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Table not initialized. [Op:LookupTableFindV2] name: \u001b[0m\n\nArguments received by TextVectorization.call():\n  â€¢ inputs=[\"'ooooo'\"]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[427], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m new_abstract \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mooooo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m predicted_category \u001b[38;5;241m=\u001b[39m predict_category(new_abstract, loaded_model, loaded_text_vectorizer, invert_multi_hot)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Categories:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predicted_category)\n",
      "Cell \u001b[1;32mIn[425], line 2\u001b[0m, in \u001b[0;36mpredict_category\u001b[1;34m(abstract, model, vectorizer, label_lookup)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_category\u001b[39m(abstract, model, vectorizer, label_lookup):\n\u001b[1;32m----> 2\u001b[0m     preprocessed_abstract \u001b[38;5;241m=\u001b[39m vectorizer([abstract])\n\u001b[0;32m      3\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(preprocessed_abstract)\n\u001b[0;32m      4\u001b[0m     predicted_labels \u001b[38;5;241m=\u001b[39m label_lookup(np\u001b[38;5;241m.\u001b[39mround(predictions)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\index_lookup.py:769\u001b[0m, in \u001b[0;36mIndexLookup._lookup_dense\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     lookups \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros_like(inputs, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value_dtype)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     lookups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup_table\u001b[38;5;241m.\u001b[39mlookup(inputs)\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     mask_locations \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mequal(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_key)\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Exception encountered when calling TextVectorization.call().\n\n\u001b[1m{{function_node __wrapped__LookupTableFindV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Table not initialized. [Op:LookupTableFindV2] name: \u001b[0m\n\nArguments received by TextVectorization.call():\n  â€¢ inputs=[\"'ooooo'\"]"
     ]
    }
   ],
   "source": [
    "new_abstract = \"ooooo\"\n",
    "predicted_category = predict_category(new_abstract, loaded_model, loaded_text_vectorizer, invert_multi_hot)\n",
    "print(\"Predicted Categories:\", predicted_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "c363785b-ff02-4869-9504-82e1fdbe89a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Exception encountered when calling TextVectorization.call().\n\n\u001b[1m{{function_node __wrapped__LookupTableFindV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Table not initialized. [Op:LookupTableFindV2] name: \u001b[0m\n\nArguments received by TextVectorization.call():\n  â€¢ inputs=['\\'Deep networks and decision forests (such as random forests and gradient\\\\nboosted trees) are the leading machine learning methods for structured and\\\\ntabular data, respectively. Many papers have empirically compared large numbers\\\\nof classifiers on one or two different domains (e.g., on 100 different tabular\\\\ndata settings). However, a careful conceptual and empirical comparison of these\\\\ntwo strategies using the most contemporary best practices has yet to be\\\\nperformed. Conceptually, we illustrate that both can be profitably viewed as\\\\n\"partition and vote\" schemes. Specifically, the representation space that they\\\\nboth learn is a partitioning of feature space into a union of convex polytopes.\\\\nFor inference, each decides on the basis of votes from the activated nodes.\\\\nThis formulation allows for a unified basic understanding of the relationship\\\\nbetween these methods. Empirically, we compare these two strategies on hundreds\\\\nof tabular data settings, as well as several vision and auditory settings. Our\\\\nfocus is on datasets with at most 10,000 samples, which represent a large\\\\nfraction of scientific and biomedical datasets. In general, we found forests to\\\\nexcel at tabular and structured data (vision and audition) with small sample\\\\nsizes, whereas deep nets performed better on structured data with larger sample\\\\nsizes. This suggests that further gains in both scenarios may be realized via\\\\nfurther combining aspects of forests and networks. We will continue revising\\\\nthis technical report in the coming months with updated results.\\'']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[401], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m new_abstract \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeep networks and decision forests (such as random forests and gradient\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mboosted trees) are the leading machine learning methods for structured and\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtabular data, respectively. Many papers have empirically compared large numbers\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mof classifiers on one or two different domains (e.g., on 100 different tabular\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mdata settings). However, a careful conceptual and empirical comparison of these\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtwo strategies using the most contemporary best practices has yet to be\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mperformed. Conceptually, we illustrate that both can be profitably viewed as\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartition and vote\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m schemes. Specifically, the representation space that they\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mboth learn is a partitioning of feature space into a union of convex polytopes.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFor inference, each decides on the basis of votes from the activated nodes.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThis formulation allows for a unified basic understanding of the relationship\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbetween these methods. Empirically, we compare these two strategies on hundreds\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mof tabular data settings, as well as several vision and auditory settings. Our\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfocus is on datasets with at most 10,000 samples, which represent a large\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfraction of scientific and biomedical datasets. In general, we found forests to\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mexcel at tabular and structured data (vision and audition) with small sample\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124msizes, whereas deep nets performed better on structured data with larger sample\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124msizes. This suggests that further gains in both scenarios may be realized via\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfurther combining aspects of forests and networks. We will continue revising\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mthis technical report in the coming months with updated results.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m predicted_category \u001b[38;5;241m=\u001b[39m predict_category(new_abstract, loaded_model, loaded_text_vectorizer, invert_multi_hot)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Categories:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predicted_category)\n",
      "Cell \u001b[1;32mIn[386], line 2\u001b[0m, in \u001b[0;36mpredict_category\u001b[1;34m(abstract, model, vectorizer, label_lookup)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_category\u001b[39m(abstract, model, vectorizer, label_lookup):\n\u001b[1;32m----> 2\u001b[0m     preprocessed_abstract \u001b[38;5;241m=\u001b[39m vectorizer([abstract])\n\u001b[0;32m      3\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(preprocessed_abstract)\n\u001b[0;32m      4\u001b[0m     predicted_labels \u001b[38;5;241m=\u001b[39m label_lookup(np\u001b[38;5;241m.\u001b[39mround(predictions)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\index_lookup.py:769\u001b[0m, in \u001b[0;36mIndexLookup._lookup_dense\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     lookups \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros_like(inputs, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value_dtype)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     lookups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup_table\u001b[38;5;241m.\u001b[39mlookup(inputs)\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     mask_locations \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mequal(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_key)\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Exception encountered when calling TextVectorization.call().\n\n\u001b[1m{{function_node __wrapped__LookupTableFindV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Table not initialized. [Op:LookupTableFindV2] name: \u001b[0m\n\nArguments received by TextVectorization.call():\n  â€¢ inputs=['\\'Deep networks and decision forests (such as random forests and gradient\\\\nboosted trees) are the leading machine learning methods for structured and\\\\ntabular data, respectively. Many papers have empirically compared large numbers\\\\nof classifiers on one or two different domains (e.g., on 100 different tabular\\\\ndata settings). However, a careful conceptual and empirical comparison of these\\\\ntwo strategies using the most contemporary best practices has yet to be\\\\nperformed. Conceptually, we illustrate that both can be profitably viewed as\\\\n\"partition and vote\" schemes. Specifically, the representation space that they\\\\nboth learn is a partitioning of feature space into a union of convex polytopes.\\\\nFor inference, each decides on the basis of votes from the activated nodes.\\\\nThis formulation allows for a unified basic understanding of the relationship\\\\nbetween these methods. Empirically, we compare these two strategies on hundreds\\\\nof tabular data settings, as well as several vision and auditory settings. Our\\\\nfocus is on datasets with at most 10,000 samples, which represent a large\\\\nfraction of scientific and biomedical datasets. In general, we found forests to\\\\nexcel at tabular and structured data (vision and audition) with small sample\\\\nsizes, whereas deep nets performed better on structured data with larger sample\\\\nsizes. This suggests that further gains in both scenarios may be realized via\\\\nfurther combining aspects of forests and networks. We will continue revising\\\\nthis technical report in the coming months with updated results.\\'']"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "new_abstract = 'Deep networks and decision forests (such as random forests and gradient\\nboosted trees) are the leading machine learning methods for structured and\\ntabular data, respectively. Many papers have empirically compared large numbers\\nof classifiers on one or two different domains (e.g., on 100 different tabular\\ndata settings). However, a careful conceptual and empirical comparison of these\\ntwo strategies using the most contemporary best practices has yet to be\\nperformed. Conceptually, we illustrate that both can be profitably viewed as\\n\"partition and vote\" schemes. Specifically, the representation space that they\\nboth learn is a partitioning of feature space into a union of convex polytopes.\\nFor inference, each decides on the basis of votes from the activated nodes.\\nThis formulation allows for a unified basic understanding of the relationship\\nbetween these methods. Empirically, we compare these two strategies on hundreds\\nof tabular data settings, as well as several vision and auditory settings. Our\\nfocus is on datasets with at most 10,000 samples, which represent a large\\nfraction of scientific and biomedical datasets. In general, we found forests to\\nexcel at tabular and structured data (vision and audition) with small sample\\nsizes, whereas deep nets performed better on structured data with larger sample\\nsizes. This suggests that further gains in both scenarios may be realized via\\nfurther combining aspects of forests and networks. We will continue revising\\nthis technical report in the coming months with updated results.'\n",
    "predicted_category = predict_category(new_abstract, loaded_model, loaded_text_vectorizer, invert_multi_hot)\n",
    "print(\"Predicted Categories:\", predicted_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d141c472-057e-4839-b63c-a9671e077a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcc212b3-6c58-4fb0-880c-de0e02be0e8b",
   "metadata": {},
   "source": [
    "Sentence Transformar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "daf133a6-8b69-479d-94e4-09f16aa338b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d89423f-a7e8-4dec-9777-beb4afcc6715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow<2.18,>=2.17 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tf-keras) (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jft_mahmud\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3224f88d-a7ce-4fea-8aa4-1ef7f63517e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jft_Mahmud\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jft_Mahmud\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68efffff-5c38-4edd-be5d-0274c6725da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da93736b-8f03-4d7d-a5f8-f3dedf51f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = arxiv_data['titles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c31a71fe-f2f8-45bb-8591-0f53b01e4bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d53a7cd-b773-4359-8130-882ff260d3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06643406, -0.04954603,  0.06388083, ...,  0.00106309,\n",
       "        -0.12156384, -0.06962774],\n",
       "       [ 0.09212256, -0.07606938,  0.06572868, ..., -0.08565164,\n",
       "        -0.09266549,  0.00725293],\n",
       "       [-0.08162683,  0.02428931,  0.0188875 , ...,  0.00806162,\n",
       "        -0.0512953 , -0.05873996],\n",
       "       ...,\n",
       "       [-0.09695337,  0.00057092,  0.07726488, ..., -0.01443806,\n",
       "        -0.04748214,  0.06130564],\n",
       "       [ 0.00768873, -0.1012418 ,  0.08909854, ..., -0.08199865,\n",
       "        -0.05649742,  0.09007055],\n",
       "       [ 0.06078517, -0.08312798, -0.00907767, ..., -0.03148185,\n",
       "         0.05713108,  0.05696892]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028b8231-09ce-49f5-b89e-f6f548eab95b",
   "metadata": {},
   "source": [
    "Why select all-MiniLM-L6-v2?\n",
    "All-round model tuned for many use-cases. Trained on a large and diverse dataset of over 1 billion training pairs. Source\n",
    "\n",
    "Its small in size 80 MB with good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a48199-1edd-4c9f-9b45-89b802c52917",
   "metadata": {},
   "source": [
    "Print the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "caac8abf-a38a-4e34-a29d-34b78403abe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Power up! Robust Graph Convolutional Network via Graph Powering\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Releasing Graph Neural Networks with Differential Privacy Guarantees\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Lifelong Graph Learning\n",
      "Embedding length: 384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "#This loop iterates over pairs of sentences and their corresponding embeddings. \n",
    "#zip is used to iterate over both lists simultaneously.\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding length:\", len(embedding)) # list of floats\n",
    "    # print(\"Embedding length:\",embedding) \n",
    "    print(\"\")\n",
    "    # Breaks out of the loop after printing information for the first 5 sentences.\n",
    "    if c >=5:\n",
    "        break\n",
    "    c +=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6118d1e-a790-4d54-b284-bb0848d198ce",
   "metadata": {},
   "source": [
    "Save Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4680531-47f3-43e9-a7fb-6db46a309961",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"models/embeddings.pkl\",'wb') as f:\n",
    "    pickle.dump(embeddings,f)\n",
    "    \n",
    "with open(\"models/sentences.pkl\",'wb') as f:\n",
    "    pickle.dump(sentences,f)\n",
    "    \n",
    "with open(\"models/rec_model.pkl\",'wb') as f:\n",
    "    pickle.dump(model,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b76c1e7-188b-4797-9031-2bfa0bab133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load save files\n",
    "embeddings = pickle.load(open('models/embeddings.pkl','rb'))\n",
    "sentences = pickle.load(open('models/sentences.pkl','rb'))\n",
    "rec_model = pickle.load(open('models/rec_model.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b2084-6732-471b-84f2-c0e89971c05e",
   "metadata": {},
   "source": [
    "Recommendation for Similar Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11605f9d-93fa-4fd9-a535-15cf698b6b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "embeddings = pickle.load(open('models/embeddings.pkl','rb'))\n",
    "sentences = pickle.load(open('models/sentences.pkl','rb'))\n",
    "rec_model = pickle.load(open('models/rec_model.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c9807f8e-5e5b-4d0b-be03-7ed775a61e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def recommendation(input_paper):\n",
    "    # Calculate cosine similarity scores between the embeddings of input_paper and all papers in the dataset.\n",
    "    cosine_scores = util.cos_sim(embeddings, rec_model.encode(input_paper))\n",
    "    \n",
    "    # Get the indices of the top-k most similar papers based on cosine similarity.\n",
    "    top_similar_papers = torch.topk(cosine_scores, dim=0, k=5, sorted=True)\n",
    "                                 \n",
    "    # Retrieve the titles of the top similar papers.\n",
    "    papers_list = []\n",
    "    for i in top_similar_papers.indices:\n",
    "        papers_list.append(sentences[i.item()])\n",
    "    \n",
    "    return papers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9f4ee5ef-c3cc-4353-b092-bb30d9c7b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_paper = input(\"Enter research paper title.....\")\n",
    "# recommend_papers = recommendation(input_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9f3c0d23-10a5-45d2-b8ac-2cdb060519a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a= \" Attention is all you need\"\n",
    "# myemb = rec_model.encode(a)\n",
    "# util.cos_sim(embedding, myemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "47a9e5b1-e320-4728-b0b1-b408ebea7bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine_scores = util.cos_sim(embedding, rec_model.encode(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "abada2f8-13e4-498b-88e3-8f9658aec4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the title of any paper you like Attention is All you Need\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We recommend to read this paper............\n",
      "=============================================\n",
      "Attention that does not Explain Away\n",
      "Attention that does not Explain Away\n",
      "Attention that does not Explain Away\n",
      "Area Attention\n",
      "Area Attention\n"
     ]
    }
   ],
   "source": [
    "# exampel usage 1: (use this paper as input (Attention is All you Need))\n",
    "input_paper = input(\"Enter the title of any paper you like\")\n",
    "recommend_papers = recommendation(input_paper)\n",
    "\n",
    "\n",
    "print(\"We recommend to read this paper............\")\n",
    "print(\"=============================================\")\n",
    "for paper in recommend_papers:\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "73858ae9-f1c2-4968-8e36-fe561e230a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the title of any paper you like BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We recommend to read this paper............\n",
      "=============================================\n",
      "BEiT: BERT Pre-Training of Image Transformers\n",
      "BEiT: BERT Pre-Training of Image Transformers\n",
      "VL-BERT: Pre-training of Generic Visual-Linguistic Representations\n",
      "Sketch-BERT: Learning Sketch Bidirectional Encoder Representation from Transformers by Self-supervised Learning of Sketch Gestalt\n",
      "Sketch-BERT: Learning Sketch Bidirectional Encoder Representation from Transformers by Self-supervised Learning of Sketch Gestalt\n"
     ]
    }
   ],
   "source": [
    "# exampel usage 2: (use this paper as input (BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding))\n",
    "input_paper = input(\"Enter the title of any paper you like\")\n",
    "recommend_papers = recommendation(input_paper)\n",
    "\n",
    "\n",
    "print(\"We recommend to read this paper............\")\n",
    "print(\"=============================================\")\n",
    "for paper in recommend_papers:\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6c6d84b4-5398-48e8-a09f-8b0a3f653d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the title of any paper you like Review of deep learning: concepts, CNN architectures, challenges, applications, future directions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We recommend to read this paper............\n",
      "=============================================\n",
      "A Review of Deep Learning with Special Emphasis on Architectures, Applications and Recent Trends\n",
      "Review of Deep Learning\n",
      "Deep Convolutional Neural Networks: A survey of the foundations, selected improvements, and some current applications\n",
      "A Survey of the Recent Architectures of Deep Convolutional Neural Networks\n",
      "A Survey of the Recent Architectures of Deep Convolutional Neural Networks\n"
     ]
    }
   ],
   "source": [
    "# exampel usage 3: (use this paper as input (Review of deep learning: concepts, CNN architectures, challenges, applications, future directions))\n",
    "input_paper = input(\"Enter the title of any paper you like\")\n",
    "recommend_papers = recommendation(input_paper)\n",
    "\n",
    "\n",
    "print(\"We recommend to read this paper............\")\n",
    "print(\"=============================================\")\n",
    "for paper in recommend_papers:\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e850f657-10aa-44b6-a053-4317ccb38120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0+cpu\n",
      "3.2.1\n",
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "# install tool versions\n",
    "import sentence_transformers\n",
    "import tensorflow\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(sentence_transformers.__version__)\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db229f99-40f0-43e3-9277-c494059fceb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jft_Mahmud\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jft_Mahmud\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2.5.0+cpu\n",
      "3.2.1\n",
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "# install tool versions\n",
    "import sentence_transformers\n",
    "import tensorflow\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(sentence_transformers.__version__)\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f827c1c-41a2-4053-8933-58e808bbacd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
